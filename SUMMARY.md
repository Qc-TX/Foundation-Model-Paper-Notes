# Table of contents

* [👋 Qc's Research Notes](README.md)
* [LLM & Security](llm-and-security/README.md)
  * [MM-LLM](llm-and-security/readme-1/README.md)
    * [MM-LLMs: Recent Advances in MultiModal Large Language Models](llm-and-security/readme-1/mm-llms-recent-advances-in-multimodal-large-language-models.md)
    * [Multimodal datasets: misogyny, pornography, and malignant stereotypes](llm-and-security/readme-1/multimodal-datasets-misogyny-pornography-and-malignant-stereotypes.md)
    * [Sight Beyond Text: Multi-Modal Training Enhances LLMsinTruthfulness and Ethics](llm-and-security/readme-1/sight-beyond-text-multi-modal-training-enhances-llmsintruthfulness-and-ethics.md)
    * [FOUNDATION MODELS AND FAIR USE](llm-and-security/readme-1/foundation-models-and-fair-use.md)
  * [VLM-Defense](llm-and-security/vlm-defense/README.md)
    * [Removing NSFW Concepts from Vision-and-Language Models for Text-to-Image Retrieval and Generation](llm-and-security/vlm-defense/removing-nsfw-concepts-from-vision-and-language-models-for-text-to-image-retrieval-and-generation.md)
    * [Safety Alignment for Vision Language Models](llm-and-security/vlm-defense/safety-alignment-for-vision-language-models.md)
    * [AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Sh](llm-and-security/vlm-defense/adashield-safeguarding-multimodal-large-language-models-from-structure-based-attack-via-adaptive-sh.md)
    * [Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation](llm-and-security/vlm-defense/eyes-closed-safety-on-protecting-multimodal-llms-via-image-to-text-transformation.md)
    * [MLLM-Protector: Ensuring MLLM’s Safety without Hurting Performance](llm-and-security/vlm-defense/mllm-protector-ensuring-mllms-safety-without-hurting-performance.md)
    * [Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models](llm-and-security/vlm-defense/safety-fine-tuning-at-almost-no-cost-a-baseline-for-vision-large-language-models.md)
    * [SAFEGEN: Mitigating Unsafe Content Generation in Text-to-Image Models](llm-and-security/vlm-defense/safegen-mitigating-unsafe-content-generation-in-text-to-image-models.md)
    * [Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision](llm-and-security/vlm-defense/moderating-illicit-online-image-promotion-for-unsafe-user-generated-content-games-using-large-vision.md)
    * [Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding](llm-and-security/vlm-defense/mitigating-hallucinations-in-large-vision-language-models-with-instruction-contrastive-decoding.md)
    * [Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation](llm-and-security/vlm-defense/eyes-closed-safety-on-protecting-multimodal-llms-via-image-to-text-transformation-1.md)
    * [UNDERSTANDING ZERO-SHOT ADVERSARIAL ROBUSTNESS FOR LARGE-SCALE MODELS](llm-and-security/vlm-defense/understanding-zero-shot-adversarial-robustness-for-large-scale-models.md)
    * [A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection](llm-and-security/vlm-defense/a-mutation-based-method-for-multi-modal-jailbreaking-attack-detection.md)
    * [UNDERSTANDING ZERO-SHOT ADVERSARIAL ROBUSTNESS FOR LARGE-SCALE MODELS](llm-and-security/vlm-defense/understanding-zero-shot-adversarial-robustness-for-large-scale-models-1.md)
    * [AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Sh](llm-and-security/vlm-defense/adashield-safeguarding-multimodal-large-language-models-from-structure-based-attack-via-adaptive-s-1.md)
    * [CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning](llm-and-security/vlm-defense/cleanclip-mitigating-data-poisoning-attacks-in-multimodal-contrastive-learning.md)
    * [Image Safeguarding: Reasoning with Conditional Vision Language Model and Obfuscating Unsafe Content](llm-and-security/vlm-defense/image-safeguarding-reasoning-with-conditional-vision-language-model-and-obfuscating-unsafe-content.md)
    * [Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models](llm-and-security/vlm-defense/towards-safe-self-distillation-of-internet-scale-text-to-image-diffusion-models.md)
    * [Typographic Attacks in Large Multimodal Models Can be Alleviated by More Informative Prompts](llm-and-security/vlm-defense/typographic-attacks-in-large-multimodal-models-can-be-alleviated-by-more-informative-prompts.md)
    * [Onthe Robustness of Large Multimodal Models Against Image Adversarial Attacks](llm-and-security/vlm-defense/onthe-robustness-of-large-multimodal-models-against-image-adversarial-attacks.md)
    * [Removing NSFW Concepts from Vision-and-Language Models for Text-to-Image Retrieval and Generation](llm-and-security/vlm-defense/removing-nsfw-concepts-from-vision-and-language-models-for-text-to-image-retrieval-and-generation-1.md)
    * [Safety Fine-Tuning at (Almost) No Cost: ABaseline for Vision Large Language Models](llm-and-security/vlm-defense/safety-fine-tuning-at-almost-no-cost-abaseline-for-vision-large-language-models.md)
    * [Partially Recentralization Softmax Loss for Vision-Language Models Robustness](llm-and-security/vlm-defense/partially-recentralization-softmax-loss-for-vision-language-models-robustness.md)
    * [Adversarial Prompt Tuning for Vision-Language Models](llm-and-security/vlm-defense/adversarial-prompt-tuning-for-vision-language-models.md)
    * [Defense-Prefix for Preventing Typographic Attacks on CLIP](llm-and-security/vlm-defense/defense-prefix-for-preventing-typographic-attacks-on-clip.md)
    * [RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedb](llm-and-security/vlm-defense/rlhf-v-towards-trustworthy-mllms-via-behavior-alignment-from-fine-grained-correctional-human-feedb.md)
    * [AMutation-Based Method for Multi-Modal Jailbreaking Attack](llm-and-security/vlm-defense/amutation-based-method-for-multi-modal-jailbreaking-attack.md)
    * [HowEasy is It to Fool Your Multimodal LLMs? AnEmpirical Analysis on Deceptive Prompts](llm-and-security/vlm-defense/howeasy-is-it-to-fool-your-multimodal-llms-anempirical-analysis-on-deceptive-prompts.md)
    * [MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance](llm-and-security/vlm-defense/mllm-protector-ensuring-mllms-safety-without-hurting-performance-1.md)
    * [EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large](llm-and-security/vlm-defense/efuf-efficient-fine-grained-unlearning-framework-for-mitigating-hallucinations-in-multimodal-large.md)
    * [Aligning Modalities in Vision Large Language Models via Preference Fine-tuning](llm-and-security/vlm-defense/aligning-modalities-in-vision-large-language-models-via-preference-fine-tuning.md)
    * [Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Lang](llm-and-security/vlm-defense/robust-clip-unsupervised-adversarial-fine-tuning-of-vision-embeddings-for-robust-large-vision-lang.md)
    * [Machine Vision Therapy: Multimodal Large Language Models Can Enhance Visual Robustness via Denoisi](llm-and-security/vlm-defense/machine-vision-therapy-multimodal-large-language-models-can-enhance-visual-robustness-via-denoisi.md)
    * [Robust Contrastive Language-Image Pre-training against Data Poisoning and Backdoor Attacks](llm-and-security/vlm-defense/robust-contrastive-language-image-pre-training-against-data-poisoning-and-backdoor-attacks.md)
    * [HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data](llm-and-security/vlm-defense/hallucidoctor-mitigating-hallucinatory-toxicity-in-visual-instruction-data.md)
  * [VLM](llm-and-security/vlm/README.md)
    * [Scalable Performance Analysis for Vision-Language Models](llm-and-security/vlm/scalable-performance-analysis-for-vision-language-models.md)
  * [VLM-Attack](llm-and-security/vlm-attack/README.md)
    * [Circumventing Concept Erasure Methods For Text-to-Image Generative Models](llm-and-security/vlm-attack/circumventing-concept-erasure-methods-for-text-to-image-generative-models.md)
    * [Efficient LLM-Jailbreaking by Introducing Visual Modality](llm-and-security/vlm-attack/efficient-llm-jailbreaking-by-introducing-visual-modality.md)
    * [From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking](llm-and-security/vlm-attack/from-llms-to-mllms-exploring-the-landscape-of-multimodal-jailbreaking.md)
    * [Adversarial Attacks on Multimodal Agents](llm-and-security/vlm-attack/adversarial-attacks-on-multimodal-agents.md)
    * [Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Ima](llm-and-security/vlm-attack/visual-roleplay-universal-jailbreak-attack-on-multimodal-large-language-models-via-role-playing-ima.md)
    * [Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models](llm-and-security/vlm-attack/cross-modality-jailbreak-and-mismatched-attacks-on-medical-multimodal-large-language-models.md)
    * [Typography Leads Semantic Diversifying: Amplifying Adversarial Transferability across Multimodal Lar](llm-and-security/vlm-attack/typography-leads-semantic-diversifying-amplifying-adversarial-transferability-across-multimodal-lar.md)
    * [White-box Multimodal Jailbreaks Against Large Vision-Language Models](llm-and-security/vlm-attack/white-box-multimodal-jailbreaks-against-large-vision-language-models.md)
    * [Red Teaming Visual Language Models](llm-and-security/vlm-attack/red-teaming-visual-language-models.md)
    * [Private Attribute Inference from Images with Vision-Language Models](llm-and-security/vlm-attack/private-attribute-inference-from-images-with-vision-language-models.md)
    * [Assessment of Multimodal Large Language Models in Alignment with Human Values](llm-and-security/vlm-attack/assessment-of-multimodal-large-language-models-in-alignment-with-human-values.md)
    * [Privacy-Aware Visual Language Models](llm-and-security/vlm-attack/privacy-aware-visual-language-models.md)
    * [Learning To See But Forgetting To Follow: Visual Instruction Tuning Makes LLMs More Prone To Jailbre](llm-and-security/vlm-attack/learning-to-see-but-forgetting-to-follow-visual-instruction-tuning-makes-llms-more-prone-to-jailbre.md)
    * [Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks](llm-and-security/vlm-attack/vision-llms-can-fool-themselves-with-self-generated-typographic-attacks.md)
    * [Red Teaming Visual Language Models](llm-and-security/vlm-attack/red-teaming-visual-language-models-1.md)
    * [Adversarial Illusions in Multi-Modal Embeddings](llm-and-security/vlm-attack/adversarial-illusions-in-multi-modal-embeddings.md)
    * [Universal Prompt Optimizer for Safe Text-to-Image Generation](llm-and-security/vlm-attack/universal-prompt-optimizer-for-safe-text-to-image-generation.md)
    * [On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts](llm-and-security/vlm-attack/on-the-proactive-generation-of-unsafe-images-from-text-to-image-models-using-benign-prompts.md)
    * [Adversarial Illusions in Multi-Modal Embeddings](llm-and-security/vlm-attack/adversarial-illusions-in-multi-modal-embeddings-1.md)
    * [Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images](llm-and-security/vlm-attack/stop-reasoning-when-multimodal-llms-with-chain-of-thought-reasoning-meets-adversarial-images.md)
    * [INSTRUCTTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models](llm-and-security/vlm-attack/instructta-instruction-tuned-targeted-attack-for-large-vision-language-models.md)
    * [On the Robustness of Large Multimodal Models Against Image Adversarial Attacks](llm-and-security/vlm-attack/on-the-robustness-of-large-multimodal-models-against-image-adversarial-attacks.md)
    * [Hijacking Context in Large Multi-modal Models](llm-and-security/vlm-attack/hijacking-context-in-large-multi-modal-models.md)
    * [Transferable Multimodal Attack on Vision-Language Pre-training Models](llm-and-security/vlm-attack/transferable-multimodal-attack-on-vision-language-pre-training-models.md)
    * [Images are Achilles’ Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimoda](llm-and-security/vlm-attack/images-are-achilles-heel-of-alignment-exploiting-visual-vulnerabilities-for-jailbreaking-multimoda.md)
    * [AN IMAGE IS WORTH 1000 LIES: ADVERSARIAL TRANSFERABILITY ACROSS PROMPTS ON VISIONLANGUAGE MODELS](llm-and-security/vlm-attack/an-image-is-worth-1000-lies-adversarial-transferability-across-prompts-on-visionlanguage-models.md)
    * [Test-Time Backdoor Attacks on Multimodal Large Language Models](llm-and-security/vlm-attack/test-time-backdoor-attacks-on-multimodal-large-language-models.md)
    * [JAILBREAK IN PIECES: COMPOSITIONAL ADVERSARIAL ATTACKS ON MULTI-MODAL LANGUAGE MODELS](llm-and-security/vlm-attack/jailbreak-in-pieces-compositional-adversarial-attacks-on-multi-modal-language-models.md)
    * [Jailbreaking Attack against Multimodal Large Language Model](llm-and-security/vlm-attack/jailbreaking-attack-against-multimodal-large-language-model.md)
    * [Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts](llm-and-security/vlm-attack/jailbreaking-gpt-4v-via-self-adversarial-attacks-with-system-prompts.md)
    * [IMAGE HIJACKS: ADVERSARIAL IMAGES CAN CONTROL GENERATIVE MODELS AT RUNTIME](llm-and-security/vlm-attack/image-hijacks-adversarial-images-can-control-generative-models-at-runtime.md)
    * [VISUAL ADVERSARIAL EXAMPLES JAILBREAK ALIGNED LARGE LANGUAGE MODELS](llm-and-security/vlm-attack/visual-adversarial-examples-jailbreak-aligned-large-language-models.md)
    * [Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks](llm-and-security/vlm-attack/vision-llms-can-fool-themselves-with-self-generated-typographic-attacks-1.md)
    * [Query-Relevant Images Jailbreak Large Multi-Modal Models](llm-and-security/vlm-attack/query-relevant-images-jailbreak-large-multi-modal-models.md)
    * [Towards Adversarial Attack on Vision-Language Pre-training Models](llm-and-security/vlm-attack/towards-adversarial-attack-on-vision-language-pre-training-models.md)
    * [HowMany Are Unicorns in This Image? ASafety Evaluation Benchmark for Vision LLMs](llm-and-security/vlm-attack/howmany-are-unicorns-in-this-image-asafety-evaluation-benchmark-for-vision-llms.md)
    * [SA-Attack: Improving Adversarial Transferability of Vision-Language Pre-training Models via Self-Au](llm-and-security/vlm-attack/sa-attack-improving-adversarial-transferability-of-vision-language-pre-training-models-via-self-au.md)
    * [MISUSING TOOLS IN LARGE LANGUAGE MODELS WITH VISUAL ADVERSARIAL EXAMPLES](llm-and-security/vlm-attack/misusing-tools-in-large-language-models-with-visual-adversarial-examples.md)
    * [VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models](llm-and-security/vlm-attack/vl-trojan-multimodal-instruction-backdoor-attacks-against-autoregressive-visual-language-models.md)
    * [INSTRUCTTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models](llm-and-security/vlm-attack/instructta-instruction-tuned-targeted-attack-for-large-vision-language-models-1.md)
    * [Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Mod](llm-and-security/vlm-attack/set-level-guidance-attack-boosting-adversarial-transferability-of-vision-language-pre-training-mod.md)
    * [Shadowcast: STEALTHY DATA POISONING ATTACKS AGAINST VISION-LANGUAGE MODELS](llm-and-security/vlm-attack/shadowcast-stealthy-data-poisoning-attacks-against-vision-language-models.md)
    * [FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts](llm-and-security/vlm-attack/figstep-jailbreaking-large-vision-language-models-via-typographic-visual-prompts.md)
    * [THE WOLF WITHIN: COVERT INJECTION OF MALICE INTO MLLM SOCIETIES VIA AN MLLM OPERATIVE](llm-and-security/vlm-attack/the-wolf-within-covert-injection-of-malice-into-mllm-societies-via-an-mllm-operative.md)
    * [Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images](llm-and-security/vlm-attack/stop-reasoning-when-multimodal-llms-with-chain-of-thought-reasoning-meets-adversarial-images-1.md)
    * [Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast](llm-and-security/vlm-attack/agent-smith-a-single-image-can-jailbreak-one-million-multimodal-llm-agents-exponentially-fast.md)
    * [How Robust is Google’s Bard to Adversarial Image Attacks?](llm-and-security/vlm-attack/how-robust-is-googles-bard-to-adversarial-image-attacks.md)
    * [OnEvaluating Adversarial Robustness of Large Vision-Language Models](llm-and-security/vlm-attack/onevaluating-adversarial-robustness-of-large-vision-language-models.md)
    * [Onthe Adversarial Robustness of Multi-Modal Foundation Models](llm-and-security/vlm-attack/onthe-adversarial-robustness-of-multi-modal-foundation-models.md)
    * [Are aligned neural networks adversarially aligned?](llm-and-security/vlm-attack/are-aligned-neural-networks-adversarially-aligned.md)
    * [READING ISN’T BELIEVING: ADVERSARIAL ATTACKS ON MULTI-MODAL NEURONS](llm-and-security/vlm-attack/reading-isnt-believing-adversarial-attacks-on-multi-modal-neurons.md)
    * [Black Box Adversarial Prompting for Foundation Models](llm-and-security/vlm-attack/black-box-adversarial-prompting-for-foundation-models.md)
    * [Evaluation and Analysis of Hallucination in Large Vision-Language Models](llm-and-security/vlm-attack/evaluation-and-analysis-of-hallucination-in-large-vision-language-models.md)
    * [FOOL YOUR (VISION AND) LANGUAGE MODEL WITH EMBARRASSINGLY SIMPLE PERMUTATIONS](llm-and-security/vlm-attack/fool-your-vision-and-language-model-with-embarrassingly-simple-permutations.md)
    * [VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models](llm-and-security/vlm-attack/vl-trojan-multimodal-instruction-backdoor-attacks-against-autoregressive-visual-language-models-1.md)
    * [Transferable Multimodal Attack on Vision-Language Pre-training Models](llm-and-security/vlm-attack/transferable-multimodal-attack-on-vision-language-pre-training-models-1.md)
    * [BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning](llm-and-security/vlm-attack/badclip-dual-embedding-guided-backdoor-attack-on-multimodal-contrastive-learning.md)
    * [AdvCLIP: Downstream-agnostic Adversarial Examples in Multimodal Contrastive Learning](llm-and-security/vlm-attack/advclip-downstream-agnostic-adversarial-examples-in-multimodal-contrastive-learning.md)
  * [T2I-Attack](llm-and-security/t2i-attack/README.md)
    * [On Copyright Risks of Text-to-Image Diffusion Models](llm-and-security/t2i-attack/on-copyright-risks-of-text-to-image-diffusion-models.md)
    * [ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users](llm-and-security/t2i-attack/art-automatic-red-teaming-for-text-to-image-models-to-protect-benign-users.md)
    * [On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts](llm-and-security/t2i-attack/on-the-proactive-generation-of-unsafe-images-from-text-to-image-models-using-benign-prompts.md)
    * [Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts](llm-and-security/t2i-attack/prompting4debugging-red-teaming-text-to-image-diffusion-models-by-finding-problematic-prompts.md)
    * [SneakyPrompt: Jailbreaking Text-to-image Generative Models](llm-and-security/t2i-attack/sneakyprompt-jailbreaking-text-to-image-generative-models.md)
    * [The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breach](llm-and-security/t2i-attack/the-stronger-the-diffusion-model-the-easier-the-backdoor-data-poisoning-to-induce-copyright-breach.md)
    * [Discovering Universal Semantic Triggers for Text-to-Image Synthesis](llm-and-security/t2i-attack/discovering-universal-semantic-triggers-for-text-to-image-synthesis.md)
    * [Automatic Jailbreaking of the Text-to-Image Generative AI Systems](llm-and-security/t2i-attack/automatic-jailbreaking-of-the-text-to-image-generative-ai-systems.md)
  * [Survey](llm-and-security/survey/README.md)
    * [Generative AI Security: Challenges and Countermeasures](llm-and-security/survey/generative-ai-security-challenges-and-countermeasures.md)
    * [Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems](llm-and-security/survey/beyond-boundaries-a-comprehensive-survey-of-transferable-attacks-on-ai-systems.md)
    * [Current state of LLM Risks and AI Guardrails](llm-and-security/survey/current-state-of-llm-risks-and-ai-guardrails.md)
    * [Security of AI Agents](llm-and-security/survey/security-of-ai-agents.md)
    * [Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents](llm-and-security/survey/watch-out-for-your-agents-investigating-backdoor-threats-to-llm-based-agents.md)
    * [Exploring Vulnerabilities and Protections in Large Language Models: A Survey](llm-and-security/survey/exploring-vulnerabilities-and-protections-in-large-language-models-a-survey.md)
    * [Unveiling Hallucination in Text, Image, Video, and Audio Foundation Models: A Comprehensive Survey](llm-and-security/survey/unveiling-hallucination-in-text-image-video-and-audio-foundation-models-a-comprehensive-survey.md)
    * [Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Mode](llm-and-security/survey/unbridled-icarus-a-survey-of-the-potential-perils-of-image-inputs-in-multimodal-large-language-mode.md)
    * [SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and Improving Large Language Mode](llm-and-security/survey/safetyprompts-a-systematic-review-of-open-datasets-for-evaluating-and-improving-large-language-mode.md)
    * [Safety of Multimodal Large Language Models on Images and Text](llm-and-security/survey/safety-of-multimodal-large-language-models-on-images-and-text.md)
    * [LLM Jailbreak Attack versus Defense Techniques - A Comprehensive Study](llm-and-security/survey/llm-jailbreak-attack-versus-defense-techniques-a-comprehensive-study.md)
    * [Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks](llm-and-security/survey/survey-of-vulnerabilities-in-large-language-models-revealed-by-adversarial-attacks.md)
    * [ASurvey on Safe Multi-Modal Learning System](llm-and-security/survey/asurvey-on-safe-multi-modal-learning-system.md)
    * [TRUSTWORTHY LARGE MODELS IN VISION: A SURVEY](llm-and-security/survey/trustworthy-large-models-in-vision-a-survey.md)
    * [A Pathway Towards Responsible AI Generated Content](llm-and-security/survey/a-pathway-towards-responsible-ai-generated-content.md)
    * [A Survey of Hallucination in “Large” Foundation Models](llm-and-security/survey/a-survey-of-hallucination-in-large-foundation-models.md)
    * [An Early Categorization of Prompt Injection Attacks on Large Language Models](llm-and-security/survey/an-early-categorization-of-prompt-injection-attacks-on-large-language-models.md)
    * [Comprehensive Assessment of Jailbreak Attacks Against LLMs](llm-and-security/survey/comprehensive-assessment-of-jailbreak-attacks-against-llms.md)
    * [A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks](llm-and-security/survey/a-comprehensive-overview-of-backdoor-attacks-in-large-language-models-within-communication-networks.md)
    * [Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks](llm-and-security/survey/survey-of-vulnerabilities-in-large-language-models-revealed-by-adversarial-attacks-1.md)
    * [Adversarial Machine Learning for Social Good: Reframing the Adversary as an Ally](llm-and-security/survey/adversarial-machine-learning-for-social-good-reframing-the-adversary-as-an-ally.md)
    * [Red-Teaming for Generative AI: Silver Bullet or Security Theater?](llm-and-security/survey/red-teaming-for-generative-ai-silver-bullet-or-security-theater.md)
    * [A STRONGREJECT for Empty Jailbreaks](llm-and-security/survey/a-strongreject-for-empty-jailbreaks.md)
  * [LVM-Attack](llm-and-security/lvm-attack/README.md)
    * [Adversarial Attacks on Foundational Vision Models](llm-and-security/lvm-attack/adversarial-attacks-on-foundational-vision-models.md)
  * [For Good](llm-and-security/for-good/README.md)
    * [Image Safeguarding: Reasoning with Conditional Vision Language Model and Obfuscating Unsafe Content](llm-and-security/for-good/image-safeguarding-reasoning-with-conditional-vision-language-model-and-obfuscating-unsafe-content.md)
  * [Benchmark](llm-and-security/benchmark/README.md)
    * [HALLUSIONBENCH: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusi](llm-and-security/benchmark/hallusionbench-an-advanced-diagnostic-suite-for-entangled-language-hallucination-and-visual-illusi.md)
    * [OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety](llm-and-security/benchmark/openeval-benchmarking-chinese-llms-across-capability-alignment-and-safety.md)
    * [ToViLaG: Your Visual-Language Generative Model is Also An Evildoer](llm-and-security/benchmark/tovilag-your-visual-language-generative-model-is-also-an-evildoer.md)
    * [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](llm-and-security/benchmark/harmbench-a-standardized-evaluation-framework-for-automated-red-teaming-and-robust-refusal.md)
    * [S-Eval: Automatic and Adaptive Test Generation for Benchmarking Safety Evaluation of Large Language](llm-and-security/benchmark/s-eval-automatic-and-adaptive-test-generation-for-benchmarking-safety-evaluation-of-large-language.md)
    * [UnsafeBench: Benchmarking Image Safety Classifiers on Real-World and AI-Generated Images](llm-and-security/benchmark/unsafebench-benchmarking-image-safety-classifiers-on-real-world-and-ai-generated-images.md)
    * [JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against](llm-and-security/benchmark/jailbreakv-28k-a-benchmark-for-assessing-the-robustness-of-multimodal-large-language-models-against.md)
    * [JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models](llm-and-security/benchmark/jailbreakbench-an-open-robustness-benchmark-for-jailbreaking-large-language-models.md)
    * [Constructing Benchmarks and Interventions for Combating Hallucinations in LLMs](llm-and-security/benchmark/constructing-benchmarks-and-interventions-for-combating-hallucinations-in-llms.md)
    * [ALERT: A Comprehensive Benchmark for Assessing Large Language Models’ Safety through Red Teaming](llm-and-security/benchmark/alert-a-comprehensive-benchmark-for-assessing-large-language-models-safety-through-red-teaming.md)
    * [Benchmarking Llama2, Mistral, Gemma and GPT for Factuality, Toxicity, Bias and Propensity for Halluc](llm-and-security/benchmark/benchmarking-llama2-mistral-gemma-and-gpt-for-factuality-toxicity-bias-and-propensity-for-halluc.md)
    * [INJECAGENT: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents](llm-and-security/benchmark/injecagent-benchmarking-indirect-prompt-injections-in-tool-integrated-large-language-model-agents.md)
    * [AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Ins](llm-and-security/benchmark/avibench-towards-evaluating-the-robustness-of-large-vision-language-model-on-adversarial-visual-ins.md)
    * [HALLUSIONBENCH: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusio](llm-and-security/benchmark/hallusionbench-an-advanced-diagnostic-suite-for-entangled-language-hallucination-and-visual-illusio.md)
    * [ALL LANGUAGES MATTER: ON THE MULTILINGUAL SAFETY OF LARGE LANGUAGE MODELS](llm-and-security/benchmark/all-languages-matter-on-the-multilingual-safety-of-large-language-models.md)
    * [Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial](llm-and-security/benchmark/why-should-adversarial-perturbations-be-imperceptible-rethink-the-research-paradigm-in-adversarial.md)
    * [Red Teaming Visual Language Models](llm-and-security/benchmark/red-teaming-visual-language-models.md)
    * [Unified Hallucination Detection for Multimodal Large Language Models](llm-and-security/benchmark/unified-hallucination-detection-for-multimodal-large-language-models.md)
    * [MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark](llm-and-security/benchmark/mllm-as-a-judge-assessing-multimodal-llm-as-a-judge-with-vision-language-benchmark.md)
    * [Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning](llm-and-security/benchmark/mitigating-hallucination-in-large-multi-modal-models-via-robust-instruction-tuning.md)
    * [CAN LANGUAGE MODELS BE INSTRUCTED TO PROTECT PERSONAL INFORMATION?](llm-and-security/benchmark/can-language-models-be-instructed-to-protect-personal-information.md)
    * [Detecting and Preventing Hallucinations in Large Vision Language Models](llm-and-security/benchmark/detecting-and-preventing-hallucinations-in-large-vision-language-models.md)
    * [DRESS : Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Lang](llm-and-security/benchmark/dress-instructing-large-vision-language-models-to-align-and-interact-with-humans-via-natural-lang.md)
    * [ToViLaG: Your Visual-Language Generative Model is Also An Evildoer](llm-and-security/benchmark/tovilag-your-visual-language-generative-model-is-also-an-evildoer-1.md)
    * [SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models](llm-and-security/benchmark/sc-safety-a-multi-round-open-ended-question-adversarial-safety-benchmark-for-large-language-models.md)
    * [PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts](llm-and-security/benchmark/promptbench-towards-evaluating-the-robustness-of-large-language-models-on-adversarial-prompts.md)
    * [Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs](llm-and-security/benchmark/do-not-answer-a-dataset-for-evaluating-safeguards-in-llms.md)
  * [Explainality](llm-and-security/explainality/README.md)
    * [Visual Explanations of Image-Text Representations via Multi-Modal Information Bottleneck Attributio](llm-and-security/explainality/visual-explanations-of-image-text-representations-via-multi-modal-information-bottleneck-attributio.md)
  * [Privacy-Defense](llm-and-security/privacy-defense/README.md)
    * [Defending Our Privacy With Backdoors](llm-and-security/privacy-defense/defending-our-privacy-with-backdoors.md)
    * [PromptCARE: Prompt Copyright Protection by Watermark Injection and Verification](llm-and-security/privacy-defense/promptcare-prompt-copyright-protection-by-watermark-injection-and-verification.md)
  * [Privacy-Attack](llm-and-security/privacy-attack/README.md)
    * [PANDORA’S WHITE-BOX: INCREASED TRAINING DATA LEAKAGE IN OPEN LLMS](llm-and-security/privacy-attack/pandoras-white-box-increased-training-data-leakage-in-open-llms.md)
    * [Untitled](llm-and-security/privacy-attack/untitled.md)
    * [Membership Inference Attacks against Large Language Models via Self-prompt Calibration](llm-and-security/privacy-attack/membership-inference-attacks-against-large-language-models-via-self-prompt-calibration.md)
    * [LANGUAGE MODEL INVERSION](llm-and-security/privacy-attack/language-model-inversion.md)
    * [Effective Prompt Extraction from Language Models](llm-and-security/privacy-attack/effective-prompt-extraction-from-language-models.md)
    * [Prompt Stealing Attacks Against Large Language Models](llm-and-security/privacy-attack/prompt-stealing-attacks-against-large-language-models.md)
    * [Stealing Part of a Production Language Model](llm-and-security/privacy-attack/stealing-part-of-a-production-language-model.md)
    * [Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Cali](llm-and-security/privacy-attack/practical-membership-inference-attacks-against-fine-tuned-large-language-models-via-self-prompt-cali.md)
    * [Prompt Stealing Attacks Against Large Language Models](llm-and-security/privacy-attack/prompt-stealing-attacks-against-large-language-models-1.md)
    * [PRSA: Prompt Reverse Stealing Attacks against Large Language Models](llm-and-security/privacy-attack/prsa-prompt-reverse-stealing-attacks-against-large-language-models.md)
    * [Low-Resource Languages Jailbreak GPT-4](llm-and-security/privacy-attack/low-resource-languages-jailbreak-gpt-4.md)
    * [Scalable Extraction of Training Data from (Production) Language Models](llm-and-security/privacy-attack/scalable-extraction-of-training-data-from-production-language-models.md)
  * [Others](llm-and-security/others/README.md)
    * [INFERRING OFFENSIVENESS IN IMAGES FROM NATURAL LANGUAGE SUPERVISION](llm-and-security/others/inferring-offensiveness-in-images-from-natural-language-supervision.md)
    * [An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulne](llm-and-security/others/an-llm-assisted-easy-to-trigger-backdoor-attack-on-code-completion-models-injecting-disguised-vulne.md)
    * [More RLHF, More Trust? On The Impact of Human Preference Alignment On Language Model Trustworthiness](llm-and-security/others/more-rlhf-more-trust-on-the-impact-of-human-preference-alignment-on-language-model-trustworthiness.md)
    * [AI SAFETY: A CLIMB TO ARMAGEDDON?](llm-and-security/others/ai-safety-a-climb-to-armageddon.md)
    * [AI RISK MANAGEMENT SHOULD INCORPORATE BOTH SAFETY AND SECURITY](llm-and-security/others/ai-risk-management-should-incorporate-both-safety-and-security.md)
    * [Defending Against Social Engineering Attacks in the Age of LLMs](llm-and-security/others/defending-against-social-engineering-attacks-in-the-age-of-llms.md)
    * [Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI](llm-and-security/others/adversarial-perturbations-cannot-reliably-protect-artists-from-generative-ai.md)
    * [Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](llm-and-security/others/training-a-helpful-and-harmless-assistant-with-reinforcement-learning-from-human-feedback.md)
    * [Deduplicating Training Data Makes Language Models Better](llm-and-security/others/deduplicating-training-data-makes-language-models-better.md)
    * [MITIGATING TEXT TOXICITY WITH COUNTERFACTUAL GENERATION](llm-and-security/others/mitigating-text-toxicity-with-counterfactual-generation.md)
    * [The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?](llm-and-security/others/the-first-to-know-how-token-distributions-reveal-hidden-knowledge-in-large-vision-language-models.md)
    * [Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval](llm-and-security/others/mitigating-hallucinations-in-large-language-models-via-self-refinement-enhanced-knowledge-retrieval.md)
    * [Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?](llm-and-security/others/does-fine-tuning-llms-on-new-knowledge-encourage-hallucinations.md)
    * [Mitigating LLM Hallucinations via Conformal Abstention](llm-and-security/others/mitigating-llm-hallucinations-via-conformal-abstention.md)
    * [Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback](llm-and-security/others/detecting-and-mitigating-hallucination-in-large-vision-language-models-via-fine-grained-ai-feedback.md)
    * [Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics](llm-and-security/others/can-chatgpt-detect-deepfakes-a-study-of-using-multimodal-large-language-models-for-media-forensics.md)
    * [An Analysis of Recent Advances in Deepfake Image Detection in an Evolving Threat Landscape](llm-and-security/others/an-analysis-of-recent-advances-in-deepfake-image-detection-in-an-evolving-threat-landscape.md)
    * [Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding](llm-and-security/others/mitigating-hallucinations-in-large-vision-language-models-with-instruction-contrastive-decoding.md)
    * [LARGE LANGUAGE MODELS AS AUTOMATED ALIGNERS FOR BENCHMARKING VISION-LANGUAGE MODELS](llm-and-security/others/large-language-models-as-automated-aligners-for-benchmarking-vision-language-models.md)
    * [PoLLMgraph: Unraveling Hallucinations in Large Language Models via State Transition Dynamics](llm-and-security/others/pollmgraph-unraveling-hallucinations-in-large-language-models-via-state-transition-dynamics.md)
    * [Reducing hallucination in structured outputs via Retrieval-Augmented Generation](llm-and-security/others/reducing-hallucination-in-structured-outputs-via-retrieval-augmented-generation.md)
    * [Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision](llm-and-security/others/moderating-illicit-online-image-promotion-for-unsafe-user-generated-content-games-using-large-vision.md)
    * [Attacking LLM Watermarks by Exploiting Their Strengths](llm-and-security/others/attacking-llm-watermarks-by-exploiting-their-strengths.md)
    * [The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Mod](llm-and-security/others/the-butterfly-effect-of-altering-prompts-how-small-changes-and-jailbreaks-affect-large-language-mod.md)
    * [TOFU: A Task of Fictitious Unlearning for LLMs](llm-and-security/others/tofu-a-task-of-fictitious-unlearning-for-llms.md)
    * [Learning and Forgetting Unsafe Examples in Large Language Models](llm-and-security/others/learning-and-forgetting-unsafe-examples-in-large-language-models.md)
    * [Exploring Adversarial Attacks against Latent Diffusion Model from the Perspective of Adversarial Tra](llm-and-security/others/exploring-adversarial-attacks-against-latent-diffusion-model-from-the-perspective-of-adversarial-tra.md)
    * [TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space](llm-and-security/others/truthx-alleviating-hallucinations-by-editing-large-language-models-in-truthful-space.md)
    * [In Search of Truth: An Interrogation Approach to Hallucination Detection](llm-and-security/others/in-search-of-truth-an-interrogation-approach-to-hallucination-detection.md)
    * [Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification](llm-and-security/others/fact-checking-the-output-of-large-language-models-via-token-level-uncertainty-quantification.md)
    * [Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models](llm-and-security/others/unsupervised-real-time-hallucination-detection-based-on-the-internal-states-of-large-language-models.md)
    * [Locating and Mitigating Gender Bias in Large Language Models](llm-and-security/others/locating-and-mitigating-gender-bias-in-large-language-models.md)
    * [Learning to Edit: Aligning LLMs with Knowledge Editing](llm-and-security/others/learning-to-edit-aligning-llms-with-knowledge-editing.md)
    * [Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding](llm-and-security/others/mitigating-hallucinations-in-large-vision-language-models-with-instruction-contrastive-decoding-1.md)
    * [Quantitative Analysis of AI-Generated Texts in Academic Research: A Study of AI Presence in Arxiv Su](llm-and-security/others/quantitative-analysis-of-ai-generated-texts-in-academic-research-a-study-of-ai-presence-in-arxiv-su.md)
    * [Does DETECTGPT Fully Utilize Perturbation? Bridge Selective Perturbation to Fine-tuned Contrastive L](llm-and-security/others/does-detectgpt-fully-utilize-perturbation-bridge-selective-perturbation-to-fine-tuned-contrastive-l.md)
    * [TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection](llm-and-security/others/teller-a-trustworthy-framework-for-explainable-generalizable-and-controllable-fake-news-detection.md)
    * [SPOTTING LLMS WITH BINOCULARS: ZERO-SHOT DETECTION OF MACHINE-GENERATED TEXT](llm-and-security/others/spotting-llms-with-binoculars-zero-shot-detection-of-machine-generated-text.md)
    * [LLM-as-a-Coauthor: The Challenges of Detecting LLM-Human Mixcase](llm-and-security/others/llm-as-a-coauthor-the-challenges-of-detecting-llm-human-mixcase.md)
    * [WHAT’S IN MY BIG DATA?](llm-and-security/others/whats-in-my-big-data.md)
    * [UNDERSTANDING CATASTROPHIC FORGETTING IN LANGUAGE MODELS VIA IMPLICIT INFERENCE](llm-and-security/others/understanding-catastrophic-forgetting-in-language-models-via-implicit-inference.md)
    * [Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models](llm-and-security/others/unsafe-diffusion-on-the-generation-of-unsafe-images-and-hateful-memes-from-text-to-image-models.md)
    * [Toxicity in CHATGPT: Analyzing Persona-assigned Language Models](llm-and-security/others/toxicity-in-chatgpt-analyzing-persona-assigned-language-models.md)
    * [MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation](llm-and-security/others/memecraft-contextual-and-stance-driven-multimodal-meme-generation.md)
    * [Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision](llm-and-security/others/moderating-illicit-online-image-promotion-for-unsafe-user-generated-content-games-using-large-visi-1.md)
    * [Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in Large Language Models](llm-and-security/others/moderating-new-waves-of-online-hate-with-chain-of-thought-reasoning-in-large-language-models.md)
    * [Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers’ Coding Practices with Insecure Sug](llm-and-security/others/poisoned-chatgpt-finds-work-for-idle-hands-exploring-developers-coding-practices-with-insecure-sug.md)
    * [Zero shot VLMs for hate meme detection: Are we there yet?](llm-and-security/others/zero-shot-vlms-for-hate-meme-detection-are-we-there-yet.md)
    * [ANALYZING AND MITIGATING OBJECT HALLUCINATION IN LARGE VISION-LANGUAGE MODELS](llm-and-security/others/analyzing-and-mitigating-object-hallucination-in-large-vision-language-models.md)
    * [MITIGATING HALLUCINATION IN LARGE MULTIMODAL MODELS VIA ROBUST INSTRUCTION TUNING](llm-and-security/others/mitigating-hallucination-in-large-multimodal-models-via-robust-instruction-tuning.md)
    * [DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCT](llm-and-security/others/denevil-towards-deciphering-and-navigating-the-ethical-values-of-large-language-models-via-instruct.md)
    * [Disentangling Perceptions of Offensiveness: Cultural and Moral Correlates](llm-and-security/others/disentangling-perceptions-of-offensiveness-cultural-and-moral-correlates.md)
    * [Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity](llm-and-security/others/red-teaming-chatgpt-via-jailbreaking-bias-robustness-reliability-and-toxicity.md)
    * [LARGE LANGUAGE MODELS AS AUTOMATED ALIGNERS FOR BENCHMARKING VISION-LANGUAGE MODELS](llm-and-security/others/large-language-models-as-automated-aligners-for-benchmarking-vision-language-models-1.md)
    * [Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Pro](llm-and-security/others/not-what-youve-signed-up-for-compromising-real-world-llm-integrated-applications-with-indirect-pro.md)
    * [InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance](llm-and-security/others/inferaligner-inference-time-alignment-for-harmlessness-through-cross-model-guidance.md)
    * [CAN LANGUAGE MODELS BE INSTRUCTED TO PROTECT PERSONAL INFORMATION?](llm-and-security/others/can-language-models-be-instructed-to-protect-personal-information.md)
    * [AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications](llm-and-security/others/aart-ai-assisted-red-teaming-with-diverse-data-generation-for-new-llm-powered-applications.md)
    * [Prompt Injection Attacks and Defenses in LLM-Integrated Applications](llm-and-security/others/prompt-injection-attacks-and-defenses-in-llm-integrated-applications.md)
    * [Removing RLHF Protections in GPT-4 via Fine-Tuning](llm-and-security/others/removing-rlhf-protections-in-gpt-4-via-fine-tuning.md)
    * [SPML: A DSL for Defending Language Models Against Prompt Attacks](llm-and-security/others/spml-a-dsl-for-defending-language-models-against-prompt-attacks.md)
    * [Stealthy Attack on Large Language Model based Recommendation](llm-and-security/others/stealthy-attack-on-large-language-model-based-recommendation.md)
    * [Large Language Models Sometimes Generate Purely Negatively-Reinforced Text](llm-and-security/others/large-language-models-sometimes-generate-purely-negatively-reinforced-text.md)
    * [On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective](llm-and-security/others/on-the-robustness-of-chatgpt-an-adversarial-and-out-of-distribution-perspective.md)
    * [Learning from data in the mixed adversarial non-adversarial case: Finding the helpers and ignoring t](llm-and-security/others/learning-from-data-in-the-mixed-adversarial-non-adversarial-case-finding-the-helpers-and-ignoring-t.md)
    * [longhorns at DADC 2022: How many linguists does it take to fool a Question Answering model? A system](llm-and-security/others/longhorns-at-dadc-2022-how-many-linguists-does-it-take-to-fool-a-question-answering-model-a-system.md)
    * [A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning](llm-and-security/others/a-prompt-array-keeps-the-bias-away-debiasing-vision-language-models-with-adversarial-learning.md)
    * [Adversarial Examples Generation for Reducing Implicit Gender Bias in Pre-trained Models](llm-and-security/others/adversarial-examples-generation-for-reducing-implicit-gender-bias-in-pre-trained-models.md)
    * [Discovering the Hidden Vocabulary of DALLE-2](llm-and-security/others/discovering-the-hidden-vocabulary-of-dalle-2.md)
    * [Raising the Cost of Malicious AI-Powered Image Editing](llm-and-security/others/raising-the-cost-of-malicious-ai-powered-image-editing.md)
    * [Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimi](llm-and-security/others/negating-negatives-alignment-without-human-positive-samples-via-distributional-dispreference-optimi.md)
    * [ALIGNERS: DECOUPLING LLMS AND ALIGNMENT](llm-and-security/others/aligners-decoupling-llms-and-alignment.md)
    * [CAN LLM-GENERATED MISINFORMATION BE DETECTED?](llm-and-security/others/can-llm-generated-misinformation-be-detected.md)
    * [On the Risk of Misinformation Pollution with Large Language Models](llm-and-security/others/on-the-risk-of-misinformation-pollution-with-large-language-models.md)
    * [Evading Watermark based Detection of AI-Generated Content](llm-and-security/others/evading-watermark-based-detection-of-ai-generated-content.md)
    * [Mitigating Inappropriateness in Image Generation: Can there be Value in Reflecting the World’s Uglin](llm-and-security/others/mitigating-inappropriateness-in-image-generation-can-there-be-value-in-reflecting-the-worlds-uglin.md)
    * [Privacy-Preserving Instructions for Aligning Large Language Models](llm-and-security/others/privacy-preserving-instructions-for-aligning-large-language-models.md)
    * [TOWARDS UNDERSTANDING THE INTERPLAY OF GENERATIVE ARTIFICIAL INTELLIGENCE AND THE INTERNET](llm-and-security/others/towards-understanding-the-interplay-of-generative-artificial-intelligence-and-the-internet.md)
    * [Evaluating the Social Impact of Generative AI Systems in Systems and Society](llm-and-security/others/evaluating-the-social-impact-of-generative-ai-systems-in-systems-and-society.md)
    * [Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities](llm-and-security/others/transformation-vs-tradition-artificial-general-intelligence-agi-for-arts-and-humanities.md)
    * [Attacking LLM Watermarks by Exploiting Their Strengths](llm-and-security/others/attacking-llm-watermarks-by-exploiting-their-strengths-1.md)
    * [TOWARDS RESPONSIBLE AI IN THE ERA OF GENERATIVE AI: A REFERENCE ARCHITECTURE FOR DESIGNING FOUNDATIO](llm-and-security/others/towards-responsible-ai-in-the-era-of-generative-ai-a-reference-architecture-for-designing-foundatio.md)
    * [RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment](llm-and-security/others/raft-reward-ranked-finetuning-for-generative-foundation-model-alignment.md)
    * [Intent-aligned AI systems deplete human agency: the need for agency foundations research in AI safet](llm-and-security/others/intent-aligned-ai-systems-deplete-human-agency-the-need-for-agency-foundations-research-in-ai-safet.md)
    * [Risk Assessment and Statistical Significance in the Age of Foundation Models](llm-and-security/others/risk-assessment-and-statistical-significance-in-the-age-of-foundation-models.md)
    * [The Foundation Model Transparency Index](llm-and-security/others/the-foundation-model-transparency-index.md)
    * [The Privacy Pillar - A Conceptual Framework for Foundation Model-based Systems](llm-and-security/others/the-privacy-pillar-a-conceptual-framework-for-foundation-model-based-systems.md)
    * [A Baseline Analysis of Reward Models’ Ability To Accurately Analyze Foundation Models Under Distribu](llm-and-security/others/a-baseline-analysis-of-reward-models-ability-to-accurately-analyze-foundation-models-under-distribu.md)
    * [Foundational Moral Values for AI Alignment](llm-and-security/others/foundational-moral-values-for-ai-alignment.md)
    * [Hazards from Increasingly Accessible Fine-Tuning of Downloadable Foundation Models](llm-and-security/others/hazards-from-increasingly-accessible-fine-tuning-of-downloadable-foundation-models.md)
    * [ON CATASTROPHIC INHERITANCE OF LARGE FOUNDATION MODELS](llm-and-security/others/on-catastrophic-inheritance-of-large-foundation-models.md)
    * [Foundation Model Sherpas: Guiding Foundation Models through Knowledge and Reasoning](llm-and-security/others/foundation-model-sherpas-guiding-foundation-models-through-knowledge-and-reasoning.md)
    * [Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustmen](llm-and-security/others/rewards-in-context-multi-objective-alignment-of-foundation-models-with-dynamic-preference-adjustmen.md)
    * [Foundation Model Transparency Reports](llm-and-security/others/foundation-model-transparency-reports.md)
    * [SECURING RELIABILITY: A BRIEF OVERVIEW ON ENHANCING IN-CONTEXT LEARNING FOR FOUNDATION MODELS](llm-and-security/others/securing-reliability-a-brief-overview-on-enhancing-in-context-learning-for-foundation-models.md)
    * [EXPLORING THE ADVERSARIAL CAPABILITIES OF LARGE LANGUAGE MODELS](llm-and-security/others/exploring-the-adversarial-capabilities-of-large-language-models.md)
    * [TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification](llm-and-security/others/trap-targeted-random-adversarial-prompt-honeypot-for-black-box-identification.md)
    * [LLM-Resistant Math Word Problem Generation via Adversarial Attacks](llm-and-security/others/llm-resistant-math-word-problem-generation-via-adversarial-attacks.md)
    * [Efficient Black-Box Adversarial Attacks on Neural Text Detectors](llm-and-security/others/efficient-black-box-adversarial-attacks-on-neural-text-detectors.md)
    * [Adversarial Preference Optimization](llm-and-security/others/adversarial-preference-optimization.md)
    * [Combating Adversarial Attacks with Multi-Agent Debate](llm-and-security/others/combating-adversarial-attacks-with-multi-agent-debate.md)
    * [How the Advent of Ubiquitous Large Language Models both Stymie and Turbocharge Dynamic Adversarial Q](llm-and-security/others/how-the-advent-of-ubiquitous-large-language-models-both-stymie-and-turbocharge-dynamic-adversarial-q.md)
    * [L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks](llm-and-security/others/l-autoda-leveraging-large-language-models-for-automated-decision-based-adversarial-attacks.md)
    * [Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection](llm-and-security/others/hidding-the-ghostwriters-an-adversarial-evaluation-of-ai-generated-student-essay-detection.md)
    * [What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detectio](llm-and-security/others/what-does-the-bot-say-opportunities-and-risks-of-large-language-models-in-social-media-bot-detectio.md)
    * [Prompted Contextual Vectors for Spear-Phishing Detection](llm-and-security/others/prompted-contextual-vectors-for-spear-phishing-detection.md)
    * [Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection](llm-and-security/others/token-ensemble-text-generation-on-attacking-the-automatic-ai-generated-text-detection.md)
    * [Recursive Chain-of-Feedback Prevents Performance Degradation from Redundant Prompting](llm-and-security/others/recursive-chain-of-feedback-prevents-performance-degradation-from-redundant-prompting.md)
    * [Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents](llm-and-security/others/watch-out-for-your-agents-investigating-backdoor-threats-to-llm-based-agents.md)
    * [RADAR: Robust AI-Text Detection via Adversarial Learning](llm-and-security/others/radar-robust-ai-text-detection-via-adversarial-learning.md)
    * [OUTFOX: LLM-Generated Essay Detection Through In-Context Learning with Adversarially Generated Examp](llm-and-security/others/outfox-llm-generated-essay-detection-through-in-context-learning-with-adversarially-generated-examp.md)
    * [Why do universal adversarial attacks work on large language models?: Geometry might be the answer](llm-and-security/others/why-do-universal-adversarial-attacks-work-on-large-language-models-geometry-might-be-the-answer.md)
    * [J-Guard: Journalism Guided Adversarially Robust Detection of AI-generated News](llm-and-security/others/j-guard-journalism-guided-adversarially-robust-detection-of-ai-generated-news.md)
    * [Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge](llm-and-security/others/distilling-adversarial-prompts-from-safety-benchmarks-report-for-the-adversarial-nibbler-challenge.md)
    * [Detoxifying Large Language Models via Knowledge Editing](llm-and-security/others/detoxifying-large-language-models-via-knowledge-editing.md)
    * [Healing Unsafe Dialogue Responses with Weak Supervision Signals](llm-and-security/others/healing-unsafe-dialogue-responses-with-weak-supervision-signals.md)
  * [LLM-Attack](llm-and-security/llm-attack/README.md)
    * [Hacc-Man: An Arcade Game for Jailbreaking LLMs](llm-and-security/llm-attack/hacc-man-an-arcade-game-for-jailbreaking-llms.md)
    * [Forcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks](llm-and-security/llm-attack/forcing-generative-models-to-degenerate-ones-the-power-of-data-poisoning-attacks.md)
    * [DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions](llm-and-security/llm-attack/deceptprompt-exploiting-llm-driven-code-generation-via-adversarial-natural-language-instructions.md)
    * [Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak](llm-and-security/llm-attack/analyzing-the-inherent-response-tendency-of-llms-real-world-instructions-driven-jailbreak.md)
    * [Hijacking Large Language Models via Adversarial In-Context Learning](llm-and-security/llm-attack/hijacking-large-language-models-via-adversarial-in-context-learning.md)
    * [Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs](llm-and-security/llm-attack/make-them-spill-the-beans-coercive-knowledge-extraction-from-production-llms.md)
    * [DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Language Models](llm-and-security/llm-attack/dala-a-distribution-aware-lora-based-adversarial-attack-against-language-models.md)
    * [FRONTIER LANGUAGE MODELS ARE NOT ROBUST TO ADVERSARIAL ARITHMETIC, OR “WHAT DO I NEED TO SAY SO YOU](llm-and-security/llm-attack/frontier-language-models-are-not-robust-to-adversarial-arithmetic-or-what-do-i-need-to-say-so-you.md)
    * [Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignm](llm-and-security/llm-attack/backdoor-activation-attack-attack-large-language-models-using-activation-steering-for-safety-alignm.md)
    * [Evil Geniuses: Delving into the Safety of LLM-based Agents](llm-and-security/llm-attack/evil-geniuses-delving-into-the-safety-of-llm-based-agents.md)
    * [BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B](llm-and-security/llm-attack/badllama-cheaply-removing-safety-fine-tuning-from-llama-2-chat-13b.md)
    * [SHADOW ALIGNMENT: THE EASE OF SUBVERTING SAFELY-ALIGNED LANGUAGE MODELS](llm-and-security/llm-attack/shadow-alignment-the-ease-of-subverting-safely-aligned-language-models.md)
    * [Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models](llm-and-security/llm-attack/prompt-as-triggers-for-backdoor-attack-examining-the-vulnerability-in-language-models.md)
    * [ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger](llm-and-security/llm-attack/chatgpt-as-an-attack-tool-stealthy-textual-backdoor-attack-via-blackbox-generative-model-trigger.md)
    * [Tastle: Distract Large Language Models for Automatic Jailbreak Attack](llm-and-security/llm-attack/tastle-distract-large-language-models-for-automatic-jailbreak-attack.md)
    * [Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue](llm-and-security/llm-attack/speak-out-of-turn-safety-vulnerability-of-large-language-models-in-multi-turn-dialogue.md)
    * [Learning to Poison Large Language Models During Instruction Tuning](llm-and-security/llm-attack/learning-to-poison-large-language-models-during-instruction-tuning.md)
    * [TALK TOO MUCH: Poisoning Large Language Models under Token Limit](llm-and-security/llm-attack/talk-too-much-poisoning-large-language-models-under-token-limit.md)
    * [Don’t Say No: Jailbreaking LLM by Suppressing Refusal](llm-and-security/llm-attack/dont-say-no-jailbreaking-llm-by-suppressing-refusal.md)
    * [Goal-guided Generative Prompt Injection Attack on Large Language Models](llm-and-security/llm-attack/goal-guided-generative-prompt-injection-attack-on-large-language-models.md)
    * [Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models](llm-and-security/llm-attack/understanding-jailbreak-success-a-study-of-latent-space-dynamics-in-large-language-models.md)
    * [BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents](llm-and-security/llm-attack/badagent-inserting-and-activating-backdoor-attacks-in-llm-agents.md)
    * [AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens](llm-and-security/llm-attack/autojailbreak-exploring-jailbreak-attacks-and-defenses-through-a-dependency-lens.md)
    * [QROA: A Black-Box Query-Response Optimization Attack on LLMs](llm-and-security/llm-attack/qroa-a-black-box-query-response-optimization-attack-on-llms.md)
    * [BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models](llm-and-security/llm-attack/badrag-identifying-vulnerabilities-in-retrieval-augmented-generation-of-large-language-models.md)
    * [Improved Generation of Adversarial Examples Against Safety-aligned LLMs](llm-and-security/llm-attack/improved-generation-of-adversarial-examples-against-safety-aligned-llms.md)
    * [Exploring Backdoor Attacks against Large Language Model-based Decision Making](llm-and-security/llm-attack/exploring-backdoor-attacks-against-large-language-model-based-decision-making.md)
    * [Jailbreak Paradox: The Achilles’ Heel of LLMs](llm-and-security/llm-attack/jailbreak-paradox-the-achilles-heel-of-llms.md)
    * [Stealth edits for provably fixing or attacking large language models](llm-and-security/llm-attack/stealth-edits-for-provably-fixing-or-attacking-large-language-models.md)
    * [Stealth edits for provably fixing or attacking large language models](llm-and-security/llm-attack/stealth-edits-for-provably-fixing-or-attacking-large-language-models-1.md)
    * [IS POISONING A REAL THREAT TO LLM ALIGNMENT? MAYBE MORE SO THAN YOU THINK](llm-and-security/llm-attack/is-poisoning-a-real-threat-to-llm-alignment-maybe-more-so-than-you-think.md)
    * [IS POISONING A REAL THREAT TO LLM ALIGNMENT? MAYBE MORE SO THAN YOU THINK](llm-and-security/llm-attack/is-poisoning-a-real-threat-to-llm-alignment-maybe-more-so-than-you-think-1.md)
    * [Knowledge-to-Jailbreak: One Knowledge Point Worth One Attack](llm-and-security/llm-attack/knowledge-to-jailbreak-one-knowledge-point-worth-one-attack.md)
    * [“Not Aligned” is Not “Malicious”: Being Careful about Hallucinations of Large Language Models’ Jailb](llm-and-security/llm-attack/not-aligned-is-not-malicious-being-careful-about-hallucinations-of-large-language-models-jailb.md)
    * [Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis](llm-and-security/llm-attack/towards-understanding-jailbreak-attacks-in-llms-a-representation-space-analysis.md)
    * [Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models](llm-and-security/llm-attack/emerging-safety-attack-and-defense-in-federated-instruction-tuning-of-large-language-models.md)
    * [Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs](llm-and-security/llm-attack/bag-of-tricks-benchmarking-of-jailbreak-attacks-on-llms.md)
    * [StructuralSleight: Automated Jailbreak Attacks on Large Language Models Utilizing Uncommon Text-Enco](llm-and-security/llm-attack/structuralsleight-automated-jailbreak-attacks-on-large-language-models-utilizing-uncommon-text-enco.md)
    * [WHEN LLM MEETS DRL: ADVANCING JAILBREAKING EFFICIENCY VIA DRL-GUIDED SEARCH](llm-and-security/llm-attack/when-llm-meets-drl-advancing-jailbreaking-efficiency-via-drl-guided-search.md)
    * [Context Injection Attacks on Large Language Models](llm-and-security/llm-attack/context-injection-attacks-on-large-language-models.md)
    * [Enhancing Jailbreak Attack Against Large Language Models through Silent Tokens](llm-and-security/llm-attack/enhancing-jailbreak-attack-against-large-language-models-through-silent-tokens.md)
    * [Phantom: General Trigger Attacks on Retrieval Augmented Language Generation](llm-and-security/llm-attack/phantom-general-trigger-attacks-on-retrieval-augmented-language-generation.md)
    * [On Trojans in Refined Language Models](llm-and-security/llm-attack/on-trojans-in-refined-language-models.md)
    * [A Survey of Backdoor Attacks and Defenses on Large Language Models: Implications for Security Measur](llm-and-security/llm-attack/a-survey-of-backdoor-attacks-and-defenses-on-large-language-models-implications-for-security-measur.md)
    * [How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States](llm-and-security/llm-attack/how-alignment-and-jailbreak-work-explain-llm-safety-through-intermediate-hidden-states.md)
    * [JAILBREAKING AS A REWARD MISSPECIFICATION PROBLEM](llm-and-security/llm-attack/jailbreaking-as-a-reward-misspecification-problem.md)
    * [ObscurePrompt: Jailbreaking Large Language Models via Obscure Inpu](llm-and-security/llm-attack/obscureprompt-jailbreaking-large-language-models-via-obscure-inpu.md)
    * [ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat Templates](llm-and-security/llm-attack/chatbug-a-common-vulnerability-of-aligned-llms-induced-by-chat-templates.md)
    * [Improved Techniques for Optimization-Based Jailbreaking on Large Language Models](llm-and-security/llm-attack/improved-techniques-for-optimization-based-jailbreaking-on-large-language-models.md)
    * [Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses](llm-and-security/llm-attack/improved-few-shot-jailbreaking-can-circumvent-aligned-language-models-and-their-defenses.md)
    * [Page 1](llm-and-security/llm-attack/page-1.md)
    * [AutoBreach: Universal and Adaptive Jailbreaking with Efficient Wordplay-Guided Optimization](llm-and-security/llm-attack/autobreach-universal-and-adaptive-jailbreaking-with-efficient-wordplay-guided-optimization.md)
    * [Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM](llm-and-security/llm-attack/chain-of-attack-a-semantic-driven-contextual-multi-turn-attacker-for-llm.md)
    * [CAN LLMS DEEPLY DETECT COMPLEX MALICIOUS QUERIES? A FRAMEWORK FOR JAILBREAKING VIA OBFUSCATING INTEN](llm-and-security/llm-attack/can-llms-deeply-detect-complex-malicious-queries-a-framework-for-jailbreaking-via-obfuscating-inten.md)
    * [SHADOW ALIGNMENT: THE EASE OF SUBVERTING SAFELY-ALIGNED LANGUAGE MODELS](llm-and-security/llm-attack/shadow-alignment-the-ease-of-subverting-safely-aligned-language-models-1.md)
    * [Hidden You Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Logic Chai](llm-and-security/llm-attack/hidden-you-malicious-goal-into-benign-narratives-jailbreak-large-language-models-through-logic-chai.md)
    * [JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models](llm-and-security/llm-attack/jailbreaklens-visual-analysis-of-jailbreak-attacks-against-large-language-models.md)
    * [AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbre](llm-and-security/llm-attack/amplegcg-learning-a-universal-and-transferable-generative-model-of-adversarial-suffixes-for-jailbre.md)
    * [SANDWICH ATTACK: MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS](llm-and-security/llm-attack/sandwich-attack-multi-language-mixture-adaptive-attack-on-llms.md)
    * [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](llm-and-security/llm-attack/jailbreaking-leading-safety-aligned-llms-with-simple-adaptive-attacks.md)
    * [Using Hallucinations to Bypass RLHF Filters](llm-and-security/llm-attack/using-hallucinations-to-bypass-rlhf-filters.md)
    * [TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4](llm-and-security/llm-attack/target-template-transferable-backdoor-attack-against-prompt-based-nlp-models-via-gpt4.md)
    * [SHADOW ALIGNMENT: THE EASE OF SUBVERTING SAFELY-ALIGNED LANGUAGE MODELS](llm-and-security/llm-attack/shadow-alignment-the-ease-of-subverting-safely-aligned-language-models-2.md)
    * [OPEN SESAME! UNIVERSAL BLACK BOX JAILBREAKING OF LARGE LANGUAGE MODELS](llm-and-security/llm-attack/open-sesame-universal-black-box-jailbreaking-of-large-language-models.md)
    * [Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts](llm-and-security/llm-attack/rainbow-teaming-open-ended-generation-of-diverse-adversarial-prompts.md)
    * [Weak-to-Strong Jailbreaking on Large Language Models](llm-and-security/llm-attack/weak-to-strong-jailbreaking-on-large-language-models.md)
    * [Punctuation Matters! Stealthy Backdoor Attack for Language Models](llm-and-security/llm-attack/punctuation-matters-stealthy-backdoor-attack-for-language-models.md)
    * [BYPASSING THE SAFETY TRAINING OF OPEN-SOURCE LLMS WITH PRIMING ATTACKS](llm-and-security/llm-attack/bypassing-the-safety-training-of-open-source-llms-with-priming-attacks.md)
    * [Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections](llm-and-security/llm-attack/stealthy-and-persistent-unalignment-on-large-language-models-via-backdoor-injections.md)
    * [A Semantic, Syntactic, And Context-Aware Natural Language Adversarial Example Generator](llm-and-security/llm-attack/a-semantic-syntactic-and-context-aware-natural-language-adversarial-example-generator.md)
    * [Fast Adversarial Attacks on Language Models In One GPU Minute](llm-and-security/llm-attack/fast-adversarial-attacks-on-language-models-in-one-gpu-minute.md)
    * [Don’t Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models](llm-and-security/llm-attack/dont-listen-to-me-understanding-and-exploring-jailbreak-prompts-of-large-language-models.md)
    * [Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks](llm-and-security/llm-attack/scaling-behavior-of-machine-translation-with-large-language-models-under-prompt-injection-attacks.md)
    * [Automatic and Universal Prompt Injection Attacks against Large Language Models](llm-and-security/llm-attack/automatic-and-universal-prompt-injection-attacks-against-large-language-models.md)
    * [Automatic and Universal Prompt Injection Attacks against Large Language Models](llm-and-security/llm-attack/automatic-and-universal-prompt-injection-attacks-against-large-language-models-1.md)
    * [Prompt Injection Attacks and Defenses in LLM-Integrated Applications](llm-and-security/llm-attack/prompt-injection-attacks-and-defenses-in-llm-integrated-applications.md)
    * [TENSOR TRUST: INTERPRETABLE PROMPT INJECTION ATTACKS FROM AN ONLINE GAME](llm-and-security/llm-attack/tensor-trust-interpretable-prompt-injection-attacks-from-an-online-game.md)
    * [DPP-Based Adversarial Prompt Searching for Lanugage Models](llm-and-security/llm-attack/dpp-based-adversarial-prompt-searching-for-lanugage-models.md)
    * [Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Agai](llm-and-security/llm-attack/large-language-models-are-better-adversaries-exploring-generative-clean-label-backdoor-attacks-agai.md)
    * [Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization](llm-and-security/llm-attack/rapid-adoption-hidden-risks-the-dual-impact-of-large-language-model-customization.md)
    * [Using Hallucinations to Bypass RLHF Filters](llm-and-security/llm-attack/using-hallucinations-to-bypass-rlhf-filters-1.md)
    * [Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks](llm-and-security/llm-attack/neural-exec-learning-and-learning-from-execution-triggers-for-prompt-injection-attacks.md)
    * [Prompt Injection attack against LLM-integrated Applications](llm-and-security/llm-attack/prompt-injection-attack-against-llm-integrated-applications.md)
    * [Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks](llm-and-security/llm-attack/scaling-behavior-of-machine-translation-with-large-language-models-under-prompt-injection-attacks-1.md)
    * [FINE-TUNING ALIGNED LANGUAGE MODELS COMPROMISES SAFETY, EVEN WHEN USERS DO NOT INTEND TO!](llm-and-security/llm-attack/fine-tuning-aligned-language-models-compromises-safety-even-when-users-do-not-intend-to.md)
    * [CATASTROPHIC JAILBREAK OF OPEN-SOURCE LLMS VIA EXPLOITING GENERATION](llm-and-security/llm-attack/catastrophic-jailbreak-of-open-source-llms-via-exploiting-generation.md)
    * [EVALUATING THE SUSCEPTIBILITY OF PRE-TRAINED LANGUAGE MODELS VIA HANDCRAFTED ADVERSARIAL EXAMPLES](llm-and-security/llm-attack/evaluating-the-susceptibility-of-pre-trained-language-models-via-handcrafted-adversarial-examples.md)
    * [Defending LLMs against Jailbreaking Attacks via Backtranslation](llm-and-security/llm-attack/defending-llms-against-jailbreaking-attacks-via-backtranslation.md)
    * [EMULATED DISALIGNMENT: SAFETY ALIGNMENT FOR LARGE LANGUAGE MODELS MAY BACKFIRE!](llm-and-security/llm-attack/emulated-disalignment-safety-alignment-for-large-language-models-may-backfire.md)
    * [GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts](llm-and-security/llm-attack/gptfuzzer-red-teaming-large-language-models-with-auto-generated-jailbreak-prompts.md)
    * [ON THE SAFETY OF OPEN-SOURCED LARGE LAN GUAGE MODELS: DOES ALIGNMENT REALLY PREVENT THEM FROM BEING](llm-and-security/llm-attack/on-the-safety-of-open-sourced-large-lan-guage-models-does-alignment-really-prevent-them-from-being.md)
    * [Unveiling the Implicit Toxicity in Large Language Models](llm-and-security/llm-attack/unveiling-the-implicit-toxicity-in-large-language-models.md)
    * [Forcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks](llm-and-security/llm-attack/forcing-generative-models-to-degenerate-ones-the-power-of-data-poisoning-attacks-1.md)
    * [Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs](llm-and-security/llm-attack/make-them-spill-the-beans-coercive-knowledge-extraction-from-production-llms-1.md)
    * [Learning to Poison Large Language Models During Instruction Tuning](llm-and-security/llm-attack/learning-to-poison-large-language-models-during-instruction-tuning-1.md)
    * [ALIGNMENT IS NOT SUFFICIENT TO PREVENT LARGE LANGUAGE MODELS FROM GENERATING HARMFUL IN FORMATION:](llm-and-security/llm-attack/alignment-is-not-sufficient-to-prevent-large-language-models-from-generating-harmful-in-formation.md)
    * [LANGUAGE MODEL UNALIGNMENT: PARAMETRIC RED-TEAMING TO EXPOSE HIDDEN HARMS AND BI ASES](llm-and-security/llm-attack/language-model-unalignment-parametric-red-teaming-to-expose-hidden-harms-and-bi-ases.md)
    * [Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections](llm-and-security/llm-attack/stealthy-and-persistent-unalignment-on-large-language-models-via-backdoor-injections-1.md)
    * [IMMUNIZATION AGAINST HARMFUL FINE-TUNING AT TACKS](llm-and-security/llm-attack/immunization-against-harmful-fine-tuning-at-tacks.md)
    * [EMULATED DISALIGNMENT: SAFETY ALIGNMENT FOR LARGE LANGUAGE MODELS MAY BACKFIRE!](llm-and-security/llm-attack/emulated-disalignment-safety-alignment-for-large-language-models-may-backfire-1.md)
    * [Composite Backdoor Attacks Against Large Language Models](llm-and-security/llm-attack/composite-backdoor-attacks-against-large-language-models.md)
    * [AWolf in Sheep’s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easi](llm-and-security/llm-attack/awolf-in-sheeps-clothing-generalized-nested-jailbreak-prompts-can-fool-large-language-models-easi.md)
    * [ALL IN HOW YOU ASK FOR IT: SIMPLE BLACK-BOX METHOD FOR JAILBREAK ATTACKS](llm-and-security/llm-attack/all-in-how-you-ask-for-it-simple-black-box-method-for-jailbreak-attacks.md)
    * [LLMJailbreak Attack versus Defense Techniques- A Comprehensive Study](llm-and-security/llm-attack/llmjailbreak-attack-versus-defense-techniques-a-comprehensive-study.md)
    * [Weak-to-Strong Jailbreaking on Large Language Models](llm-and-security/llm-attack/weak-to-strong-jailbreaking-on-large-language-models-1.md)
    * [MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds](llm-and-security/llm-attack/multiverse-exposing-large-language-model-alignment-problems-in-diverse-worlds.md)
    * [Universal and Transferable Adversarial Attacks on Aligned Language Models](llm-and-security/llm-attack/universal-and-transferable-adversarial-attacks-on-aligned-language-models.md)
    * [COERCING LLMS TO DO AND REVEAL (ALMOST) ANYTHING](llm-and-security/llm-attack/coercing-llms-to-do-and-reveal-almost-anything.md)
    * [Generating Valid and Natural Adversarial Examples with Large Language Models](llm-and-security/llm-attack/generating-valid-and-natural-adversarial-examples-with-large-language-models.md)
    * [Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections](llm-and-security/llm-attack/stealthy-and-persistent-unalignment-on-large-language-models-via-backdoor-injections-2.md)
    * [Scaling Laws for Adversarial Attacks on Language Model Activations](llm-and-security/llm-attack/scaling-laws-for-adversarial-attacks-on-language-model-activations.md)
    * [Ignore Previous Prompt: Attack Techniques For Language Models](llm-and-security/llm-attack/ignore-previous-prompt-attack-techniques-for-language-models.md)
    * [ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages](llm-and-security/llm-attack/toolsword-unveiling-safety-issues-of-large-language-models-in-tool-learning-across-three-stages.md)
    * [A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems](llm-and-security/llm-attack/a-new-era-in-llm-security-exploring-security-concerns-in-real-world-llm-based-systems.md)
    * [ATTACKING LARGE LANGUAGE MODELS WITH PROJECTED GRADIENT DESCENT](llm-and-security/llm-attack/attacking-large-language-models-with-projected-gradient-descent.md)
    * [Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack](llm-and-security/llm-attack/great-now-write-an-article-about-that-the-crescendo-multi-turn-llm-jailbreak-attack.md)
    * [Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embed](llm-and-security/llm-attack/soft-prompt-threats-attacking-safety-alignment-and-unlearning-in-open-source-llms-through-the-embed.md)
    * [Query-Based Adversarial Prompt Generation](llm-and-security/llm-attack/query-based-adversarial-prompt-generation.md)
    * [COERCING LLMS TO DO AND REVEAL (ALMOST) ANYTHING](llm-and-security/llm-attack/coercing-llms-to-do-and-reveal-almost-anything-1.md)
    * [Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment](llm-and-security/llm-attack/is-llm-as-a-judge-robust-investigating-universal-adversarial-attacks-on-zero-shot-llm-assessment.md)
    * [Fast Adversarial Attacks on Language Models In One GPU Minute](llm-and-security/llm-attack/fast-adversarial-attacks-on-language-models-in-one-gpu-minute-1.md)
    * [DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers](llm-and-security/llm-attack/drattack-prompt-decomposition-and-reconstruction-makes-powerful-llm-jailbreakers.md)
    * [From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Transla](llm-and-security/llm-attack/from-noise-to-clarity-unraveling-the-adversarial-suffix-of-large-language-model-attacks-via-transla.md)
    * [Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts](llm-and-security/llm-attack/rainbow-teaming-open-ended-generation-of-diverse-adversarial-prompts-1.md)
    * [CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models](llm-and-security/llm-attack/codechameleon-personalized-encryption-framework-for-jailbreaking-large-language-models.md)
    * [Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content](llm-and-security/llm-attack/large-language-models-are-vulnerable-to-bait-and-switch-attacks-for-generating-harmful-content.md)
    * [Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment](llm-and-security/llm-attack/mitigating-fine-tuning-jailbreak-attack-with-backdoor-enhanced-alignment.md)
    * [PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails](llm-and-security/llm-attack/prp-propagating-universal-perturbations-to-attack-large-language-model-guard-rails.md)
    * [A Cross-Language Investigation into Jailbreak Attacks in Large Language Models](llm-and-security/llm-attack/a-cross-language-investigation-into-jailbreak-attacks-in-large-language-models.md)
    * [LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario](llm-and-security/llm-attack/lora-as-an-attack-piercing-llm-safety-under-the-share-and-play-scenario.md)
    * [Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data](llm-and-security/llm-attack/best-of-venom-attacking-rlhf-by-injecting-poisoned-preference-data.md)
    * [SHORTCUTS ARISING FROM CONTRAST: EFFECTIVE AND COVERT CLEAN-LABEL ATTACKS IN PROMPT-BASED LEARNING](llm-and-security/llm-attack/shortcuts-arising-from-contrast-effective-and-covert-clean-label-attacks-in-prompt-based-learning.md)
    * [What’s in Your “Safe” Data?: Identifying Benign Data that Breaks Safety](llm-and-security/llm-attack/whats-in-your-safe-data-identifying-benign-data-that-breaks-safety.md)
    * [DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers](llm-and-security/llm-attack/drattack-prompt-decomposition-and-reconstruction-makes-powerful-llm-jailbreakers-1.md)
    * [Attacking LLM Watermarks by Exploiting Their Strengths](llm-and-security/llm-attack/attacking-llm-watermarks-by-exploiting-their-strengths.md)
    * [From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Transla](llm-and-security/llm-attack/from-noise-to-clarity-unraveling-the-adversarial-suffix-of-large-language-model-attacks-via-transl-1.md)
    * [DeepInception: Hypnotize Large Language Model to Be Jailbreaker](llm-and-security/llm-attack/deepinception-hypnotize-large-language-model-to-be-jailbreaker.md)
    * [Hijacking Large Language Models via Adversarial In-Context Learning](llm-and-security/llm-attack/hijacking-large-language-models-via-adversarial-in-context-learning-1.md)
    * [EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models](llm-and-security/llm-attack/easyjailbreak-a-unified-framework-for-jailbreaking-large-language-models.md)
    * [LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models](llm-and-security/llm-attack/linkprompt-natural-and-universal-adversarial-attacks-on-prompt-based-language-models.md)
    * [DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions](llm-and-security/llm-attack/deceptprompt-exploiting-llm-driven-code-generation-via-adversarial-natural-language-instructions-1.md)
    * [Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models](llm-and-security/llm-attack/syntactic-ghost-an-imperceptible-general-purpose-backdoor-attacks-on-pre-trained-language-models.md)
    * [Conversation Reconstruction Attack Against GPT Models](llm-and-security/llm-attack/conversation-reconstruction-attack-against-gpt-models.md)
    * [Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks](llm-and-security/llm-attack/neural-exec-learning-and-learning-from-execution-triggers-for-prompt-injection-attacks-1.md)
    * [PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models](llm-and-security/llm-attack/poisonedrag-knowledge-poisoning-attacks-to-retrieval-augmented-generation-of-large-language-models.md)
    * [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](llm-and-security/llm-attack/cold-attack-jailbreaking-llms-with-stealthiness-and-controllability.md)
    * [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit](llm-and-security/llm-attack/play-guessing-game-with-llm-indirect-jailbreak-attack-with-implicit.md)
    * [Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks](llm-and-security/llm-attack/leveraging-the-context-through-multi-round-interactions-for-jailbreaking-attacks.md)
    * [Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak](llm-and-security/llm-attack/analyzing-the-inherent-response-tendency-of-llms-real-world-instructions-driven-jailbreak-1.md)
    * [UNIVERSAL JAILBREAK BACKDOORS FROM POISONED HUMAN FEEDBACK](llm-and-security/llm-attack/universal-jailbreak-backdoors-from-poisoned-human-feedback.md)
    * [Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking](llm-and-security/llm-attack/cognitive-overload-jailbreaking-large-language-models-with-overloaded-logical-thinking.md)
    * [Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation](llm-and-security/llm-attack/scalable-and-transferable-black-box-jailbreaks-for-language-models-via-persona-modulation.md)
    * [POISONPROMPT: BACKDOOR ATTACK ON PROMPT-BASED LARGE LANGUAGE MODELS](llm-and-security/llm-attack/poisonprompt-backdoor-attack-on-prompt-based-large-language-models.md)
    * [BACKDOORING INSTRUCTION-TUNED LARGE LANGUAGE MODELS WITH VIRTUAL PROMPT INJECTION](llm-and-security/llm-attack/backdooring-instruction-tuned-large-language-models-with-virtual-prompt-injection.md)
    * [Backdoor Attacks for In-Context Learning with Language Models](llm-and-security/llm-attack/backdoor-attacks-for-in-context-learning-with-language-models.md)
    * [Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models](llm-and-security/llm-attack/instructions-as-backdoors-backdoor-vulnerabilities-of-instruction-tuning-for-large-language-models.md)
    * [UOR: Universal Backdoor Attacks on Pre-trained Language Models](llm-and-security/llm-attack/uor-universal-backdoor-attacks-on-pre-trained-language-models.md)
    * [Fake Alignment: Are LLMs Really Aligned Well?](llm-and-security/llm-attack/fake-alignment-are-llms-really-aligned-well.md)
    * [Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models](llm-and-security/llm-attack/syntactic-ghost-an-imperceptible-general-purpose-backdoor-attacks-on-pre-trained-language-models-1.md)
    * [Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignm](llm-and-security/llm-attack/backdoor-activation-attack-attack-large-language-models-using-activation-steering-for-safety-align-1.md)
    * [Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control](llm-and-security/llm-attack/imperio-language-guided-backdoor-attacks-for-arbitrary-model-control.md)
    * [Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Agai](llm-and-security/llm-attack/large-language-models-are-better-adversaries-exploring-generative-clean-label-backdoor-attacks-aga-1.md)
    * [Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning](llm-and-security/llm-attack/universal-vulnerabilities-in-large-language-models-backdoor-attacks-for-in-context-learning.md)
    * [BADCHAIN: BACKDOOR CHAIN-OF-THOUGHT PROMPTING FOR LARGE LANGUAGE MODELS](llm-and-security/llm-attack/badchain-backdoor-chain-of-thought-prompting-for-large-language-models.md)
    * [AUTODAN: INTERPRETABLE GRADIENT-BASED ADVERSARIAL ATTACKS ON LARGE LANGUAGE MODELS](llm-and-security/llm-attack/autodan-interpretable-gradient-based-adversarial-attacks-on-large-language-models.md)
    * [AN LLM CAN FOOL ITSELF: A PROMPT-BASED ADVERSARIAL ATTACK](llm-and-security/llm-attack/an-llm-can-fool-itself-a-prompt-based-adversarial-attack.md)
    * [AUTOMATIC HALLUCINATION ASSESSMENT FOR ALIGNED LARGE LANGUAGE MODELS VIA TRANSFERABLE ADVERSARIAL AT](llm-and-security/llm-attack/automatic-hallucination-assessment-for-aligned-large-language-models-via-transferable-adversarial-at.md)
    * [LLM LIES: HALLUCINATIONS ARE NOT BUGS, BUT FEATURES AS ADVERSARIAL EXAMPLES](llm-and-security/llm-attack/llm-lies-hallucinations-are-not-bugs-but-features-as-adversarial-examples.md)
    * [LOFT: LOCAL PROXY FINE-TUNING FOR IMPROVING TRANSFERABILITY OF ADVERSARIAL ATTACKS AGAINST LARGE LAN](llm-and-security/llm-attack/loft-local-proxy-fine-tuning-for-improving-transferability-of-adversarial-attacks-against-large-lan.md)
    * [Universal and Transferable Adversarial Attacks on Aligned Language Models](llm-and-security/llm-attack/universal-and-transferable-adversarial-attacks-on-aligned-language-models-1.md)
    * [Robustness Over Time: Understanding Adversarial Examples’ Effectiveness on Longitudinal Versions of](llm-and-security/llm-attack/robustness-over-time-understanding-adversarial-examples-effectiveness-on-longitudinal-versions-of.md)
    * [BASELINE DEFENSES FOR ADVERSARIAL ATTACKS AGAINST ALIGNED LANGUAGE MODELS](llm-and-security/llm-attack/baseline-defenses-for-adversarial-attacks-against-aligned-language-models.md)
    * [Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue](llm-and-security/llm-attack/speak-out-of-turn-safety-vulnerability-of-large-language-models-in-multi-turn-dialogue-1.md)
    * [Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Recon](llm-and-security/llm-attack/making-them-ask-and-answer-jailbreaking-large-language-models-in-few-queries-via-disguise-and-recon.md)
    * [Adversarial Demonstration Attacks on Large Language Models](llm-and-security/llm-attack/adversarial-demonstration-attacks-on-large-language-models.md)
    * [COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models](llm-and-security/llm-attack/cover-a-heuristic-greedy-adversarial-attack-on-prompt-based-learning-in-language-models.md)
    * [The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Mod](llm-and-security/llm-attack/the-butterfly-effect-of-altering-prompts-how-small-changes-and-jailbreaks-affect-large-language-mod.md)
    * [Open the Pandora’s Box of LLMs: Jailbreaking LLMs through Representation Engineering](llm-and-security/llm-attack/open-the-pandoras-box-of-llms-jailbreaking-llms-through-representation-engineering.md)
    * [How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Huma](llm-and-security/llm-attack/how-johnny-can-persuade-llms-to-jailbreak-them-rethinking-persuasion-to-challenge-ai-safety-by-huma.md)
    * [Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models](llm-and-security/llm-attack/sowing-the-wind-reaping-the-whirlwind-the-impact-of-editing-language-models.md)
    * [PANDORA: Jailbreak GPTs by Retrieval Augmented Generation Poisoning](llm-and-security/llm-attack/pandora-jailbreak-gpts-by-retrieval-augmented-generation-poisoning.md)
    * [Weak-to-Strong Jailbreaking on Large Language Models](llm-and-security/llm-attack/weak-to-strong-jailbreaking-on-large-language-models-2.md)
    * [Jailbreaking Proprietary Large Language Models using Word Substitution Cipher](llm-and-security/llm-attack/jailbreaking-proprietary-large-language-models-using-word-substitution-cipher.md)
    * [Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs](llm-and-security/llm-attack/semantic-mirror-jailbreak-genetic-algorithm-based-jailbreak-prompts-against-open-source-llms.md)
    * [Adversarial Attacks and Defenses in Large Language Models: Old and New Threats](llm-and-security/llm-attack/adversarial-attacks-and-defenses-in-large-language-models-old-and-new-threats.md)
    * [Jailbroken: How Does LLM Safety Training Fail?](llm-and-security/llm-attack/jailbroken-how-does-llm-safety-training-fail.md)
    * [ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs](llm-and-security/llm-attack/artprompt-ascii-art-based-jailbreak-attacks-against-aligned-llms.md)
    * [GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large](llm-and-security/llm-attack/guard-role-playing-to-generate-natural-language-jailbreakings-to-test-guideline-adherence-of-large.md)
    * [Tastle: Distract Large Language Models for Automatic Jailbreak Attack](llm-and-security/llm-attack/tastle-distract-large-language-models-for-automatic-jailbreak-attack-1.md)
    * [Exploring Safety Generalization Challenges of Large Language Models via Code](llm-and-security/llm-attack/exploring-safety-generalization-challenges-of-large-language-models-via-code.md)
    * [Learning to Poison Large Language Models During Instruction Tuning](llm-and-security/llm-attack/learning-to-poison-large-language-models-during-instruction-tuning-2.md)
    * [BADEDIT: BACKDOORING LARGE LANGUAGE MODELS BY MODEL EDITING](llm-and-security/llm-attack/badedit-backdooring-large-language-models-by-model-editing.md)
    * [Composite Backdoor Attacks Against Large Language Models](llm-and-security/llm-attack/composite-backdoor-attacks-against-large-language-models-1.md)
    * [LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario](llm-and-security/llm-attack/lora-as-an-attack-piercing-llm-safety-under-the-share-and-play-scenario-1.md)
    * [Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections](llm-and-security/llm-attack/stealthy-and-persistent-unalignment-on-large-language-models-via-backdoor-injections-3.md)
    * [ALL IN HOW YOU ASK FOR IT: SIMPLE BLACK-BOX METHOD FOR JAILBREAK ATTACKS](llm-and-security/llm-attack/all-in-how-you-ask-for-it-simple-black-box-method-for-jailbreak-attacks-1.md)
    * [THE POISON OF ALIGNMENT](llm-and-security/llm-attack/the-poison-of-alignment.md)
    * [The Philosopher’s Stone: Trojaning Plugins of Large Language Models](llm-and-security/llm-attack/the-philosophers-stone-trojaning-plugins-of-large-language-models.md)
    * [RAPID OPTIMIZATION FOR JAILBREAKING LLMS VIA SUBCONSCIOUS EXPLOITATION AND ECHOPRAXIA](llm-and-security/llm-attack/rapid-optimization-for-jailbreaking-llms-via-subconscious-exploitation-and-echopraxia.md)
    * [Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks](llm-and-security/llm-attack/leveraging-the-context-through-multi-round-interactions-for-jailbreaking-attacks-1.md)
    * [RED TEAMING GPT-4V: ARE GPT-4V SAFE AGAINST UNI/MULTI-MODAL JAILBREAK ATTACKS ?](llm-and-security/llm-attack/red-teaming-gpt-4v-are-gpt-4v-safe-against-uni-multi-modal-jailbreak-attacks.md)
    * [PAL: Proxy-Guided Black-Box Attack on Large Language Models](llm-and-security/llm-attack/pal-proxy-guided-black-box-attack-on-large-language-models.md)
    * [INCREASED LLM VULNERABILITIES FROM FINETUNING AND QUANTIZATION](llm-and-security/llm-attack/increased-llm-vulnerabilities-from-finetuning-and-quantization.md)
    * [Rethinking How to Evaluate Language Model Jailbreak](llm-and-security/llm-attack/rethinking-how-to-evaluate-language-model-jailbreak.md)
    * [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](llm-and-security/llm-attack/cold-attack-jailbreaking-llms-with-stealthiness-and-controllability-1.md)
    * [GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation](llm-and-security/llm-attack/gpt-4-jailbreaks-itself-with-near-perfect-success-using-self-explanation.md)
    * [Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation](llm-and-security/llm-attack/lockpicking-llms-a-logit-based-jailbreak-using-token-level-manipulation.md)
    * [Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs](llm-and-security/llm-attack/competition-report-finding-universal-jailbreak-backdoors-in-aligned-llms.md)
    * [AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs](llm-and-security/llm-attack/advprompter-fast-adaptive-adversarial-prompting-for-llms.md)
    * [Universal Adversarial Triggers Are Not Universal](llm-and-security/llm-attack/universal-adversarial-triggers-are-not-universal.md)
    * [PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails](llm-and-security/llm-attack/prp-propagating-universal-perturbations-to-attack-large-language-model-guard-rails-1.md)
  * [LLM-Defense](llm-and-security/llm-defense/README.md)
    * [LANGUAGE MODELS ARE HOMER SIMPSON!](llm-and-security/llm-defense/language-models-are-homer-simpson.md)
    * [garak : A Framework for Security Probing Large Language Models](llm-and-security/llm-defense/garak-a-framework-for-security-probing-large-language-models.md)
    * [Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning](llm-and-security/llm-defense/defending-against-weight-poisoning-backdoor-attacks-for-parameter-efficient-fine-tuning.md)
    * [Trojan Detection in Large Language Models: Insights from The Trojan Detection Challenge](llm-and-security/llm-defense/trojan-detection-in-large-language-models-insights-from-the-trojan-detection-challenge.md)
    * [PromptFix: Few-shot Backdoor Removal via Adversarial Prompt Tuning](llm-and-security/llm-defense/promptfix-few-shot-backdoor-removal-via-adversarial-prompt-tuning.md)
    * [The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions](llm-and-security/llm-defense/the-instruction-hierarchy-training-llms-to-prioritize-privileged-instructions.md)
    * [BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of LLM Safeguards](llm-and-security/llm-defense/bells-a-framework-towards-future-proof-benchmarks-for-the-evaluation-of-llm-safeguards.md)
    * [Cross-Task Defense: Instruction-Tuning LLMs for Content Safety](llm-and-security/llm-defense/cross-task-defense-instruction-tuning-llms-for-content-safety.md)
    * [Efficient Adversarial Training in LLMs with Continuous Attacks](llm-and-security/llm-defense/efficient-adversarial-training-in-llms-with-continuous-attacks.md)
    * [StruQ: Defending Against Prompt Injection with Structured Queries](llm-and-security/llm-defense/struq-defending-against-prompt-injection-with-structured-queries.md)
    * [Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning](llm-and-security/llm-defense/pruning-for-protection-increasing-jailbreak-resistance-in-aligned-llms-without-fine-tuning.md)
    * [GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis](llm-and-security/llm-defense/gradsafe-detecting-unsafe-prompts-for-llms-via-safety-critical-gradient-analysis.md)
    * [Defending Jailbreak Prompts via In-Context Adversarial Game](llm-and-security/llm-defense/defending-jailbreak-prompts-via-in-context-adversarial-game.md)
    * [Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework](llm-and-security/llm-defense/bergeron-combating-adversarial-attacks-through-a-conscience-based-alignment-framework.md)
    * [Jailbreaker in Jail: Moving Target Defense for Large Language Models](llm-and-security/llm-defense/jailbreaker-in-jail-moving-target-defense-for-large-language-models.md)
    * [DEFENDING AGAINST ALIGNMENT-BREAKING ATTACKS VIA ROBUSTLY ALIGNED LLM](llm-and-security/llm-defense/defending-against-alignment-breaking-attacks-via-robustly-aligned-llm.md)
    * [Causality Analysis for Evaluating the Security of Large Language Models](llm-and-security/llm-defense/causality-analysis-for-evaluating-the-security-of-large-language-models.md)
    * [AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks](llm-and-security/llm-defense/autodefense-multi-agent-llm-defense-against-jailbreak-attacks.md)
    * [Jailbreaking is Best Solved by Definition](llm-and-security/llm-defense/jailbreaking-is-best-solved-by-definition.md)
    * [RIGORLLM: RESILIENT GUARDRAILS FOR LARGE LANGUAGE MODELS AGAINST UNDESIRED CONTENT](llm-and-security/llm-defense/rigorllm-resilient-guardrails-for-large-language-models-against-undesired-content.md)
    * [LANGUAGE MODELS ARE HOMER SIMPSON! Safety Re-Alignment of Fine-tuned Language Models through Task Ar](llm-and-security/llm-defense/language-models-are-homer-simpson-safety-re-alignment-of-fine-tuned-language-models-through-task-ar.md)
    * [Defending Against Indirect Prompt Injection Attacks With Spotlighting](llm-and-security/llm-defense/defending-against-indirect-prompt-injection-attacks-with-spotlighting.md)
    * [LLMGuard: Guarding against Unsafe LLM Behavior](llm-and-security/llm-defense/llmguard-guarding-against-unsafe-llm-behavior.md)
    * [Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations](llm-and-security/llm-defense/test-time-backdoor-mitigation-for-black-box-large-language-models-with-defensive-demonstrations.md)
    * [ON TROJAN SIGNATURES IN LARGE LANGUAGE MODELS OF CODE](llm-and-security/llm-defense/on-trojan-signatures-in-large-language-models-of-code.md)
    * [Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space](llm-and-security/llm-defense/acquiring-clean-language-models-from-backdoor-poisoned-datasets-by-downscaling-frequency-space.md)
    * [Detoxifying Large Language Models via Knowledge Editing](llm-and-security/llm-defense/detoxifying-large-language-models-via-knowledge-editing.md)
    * [MART: Improving LLM Safety with Multi-round Automatic Red-Teaming](llm-and-security/llm-defense/mart-improving-llm-safety-with-multi-round-automatic-red-teaming.md)
    * [THE POISON OF ALIGNMENT](llm-and-security/llm-defense/the-poison-of-alignment.md)
    * [ROSE: Robust Selective Fine-tuning for Pre-trained Language Models](llm-and-security/llm-defense/rose-robust-selective-fine-tuning-for-pre-trained-language-models.md)
    * [GAINING WISDOM FROM SETBACKS : ALIGNING LARGE LANGUAGE MODELS VIA MISTAKE ANALYSIS](llm-and-security/llm-defense/gaining-wisdom-from-setbacks-aligning-large-language-models-via-mistake-analysis.md)
    * [Making Harmful Behaviors Unlearnable for Large Language Models](llm-and-security/llm-defense/making-harmful-behaviors-unlearnable-for-large-language-models.md)
    * [Fake Alignment: Are LLMs Really Aligned Well?](llm-and-security/llm-defense/fake-alignment-are-llms-really-aligned-well.md)
    * [Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment](llm-and-security/llm-defense/red-teaming-large-language-models-using-chain-of-utterances-for-safety-alignment.md)
    * [Vaccine: Perturbation-aware Alignment for Large Language Model](llm-and-security/llm-defense/vaccine-perturbation-aware-alignment-for-large-language-model.md)
    * [DEFENDING LARGE LANGUAGE MODELS AGAINST JAILBREAK ATTACKS VIA SEMANTIC SMOOTHING](llm-and-security/llm-defense/defending-large-language-models-against-jailbreak-attacks-via-semantic-smoothing.md)
    * [Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement](llm-and-security/llm-defense/break-the-breakout-reinventing-lm-defense-against-jailbreak-attacks-with-self-refinement.md)
    * [DEFENDING AGAINST ALIGNMENT-BREAKING AT TACKS VIA ROBUSTLY ALIGNED LLM](llm-and-security/llm-defense/defending-against-alignment-breaking-at-tacks-via-robustly-aligned-llm.md)
    * [LLMSelf Defense: By Self Examination, LLMsKnowTheyAreBeing Tricked](llm-and-security/llm-defense/llmself-defense-by-self-examination-llmsknowtheyarebeing-tricked.md)
    * [BASELINE DEFENSES FOR ADVERSARIAL ATTACKS AGAINST ALIGNED LANGUAGE MODELS](llm-and-security/llm-defense/baseline-defenses-for-adversarial-attacks-against-aligned-language-models.md)
    * [Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment](llm-and-security/llm-defense/mitigating-fine-tuning-jailbreak-attack-with-backdoor-enhanced-alignment.md)
    * [LLMsCanDefend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper](llm-and-security/llm-defense/llmscandefend-themselves-against-jailbreaking-in-a-practical-manner-a-vision-paper.md)
    * [Detoxifying Text with MARCO: Controllable Revision with Experts and Anti-Experts](llm-and-security/llm-defense/detoxifying-text-with-marco-controllable-revision-with-experts-and-anti-experts.md)
    * [Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models](llm-and-security/llm-defense/self-destructing-models-increasing-the-costs-of-harmful-dual-uses-of-foundation-models.md)
    * [Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Lang](llm-and-security/llm-defense/whispers-that-shake-foundations-analyzing-and-mitigating-false-premise-hallucinations-in-large-lang.md)
    * [CAMOUFLAGE IS ALL YOU NEED: EVALUATING AND ENHANCING LANGUAGE MODEL ROBUSTNESS AGAINST CAMOUFLAGE AD](llm-and-security/llm-defense/camouflage-is-all-you-need-evaluating-and-enhancing-language-model-robustness-against-camouflage-ad.md)
    * [Defending Jailbreak Prompts via In-Context Adversarial Game](llm-and-security/llm-defense/defending-jailbreak-prompts-via-in-context-adversarial-game-1.md)
    * [Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement](llm-and-security/llm-defense/break-the-breakout-reinventing-lm-defense-against-jailbreak-attacks-with-self-refinement-1.md)
    * [Defending LLMs against Jailbreaking Attacks via Backtranslation](llm-and-security/llm-defense/defending-llms-against-jailbreaking-attacks-via-backtranslation.md)
    * [IMMUNIZATION AGAINST HARMFUL FINE-TUNING ATTACKS](llm-and-security/llm-defense/immunization-against-harmful-fine-tuning-attacks.md)
    * [Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield](llm-and-security/llm-defense/robust-safety-classifier-for-large-language-models-adversarial-prompt-shield.md)
    * [JAB: Joint Adversarial Prompting and Belief Augmentation](llm-and-security/llm-defense/jab-joint-adversarial-prompting-and-belief-augmentation.md)
    * [TOKEN-LEVEL ADVERSARIAL PROMPT DETECTION BASED ON PERPLEXITY MEASURES AND CONTEXTUAL INFORMATION](llm-and-security/llm-defense/token-level-adversarial-prompt-detection-based-on-perplexity-measures-and-contextual-information.md)
    * [Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks](llm-and-security/llm-defense/robust-prompt-optimization-for-defending-language-models-against-jailbreaking-attacks.md)
    * [Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning](llm-and-security/llm-defense/studious-bob-fight-back-against-jailbreaking-via-prompt-adversarial-tuning.md)
    * [Vaccine: Perturbation-aware Alignment for Large Language Model](llm-and-security/llm-defense/vaccine-perturbation-aware-alignment-for-large-language-model-1.md)
    * [Improving the Robustness of Large Language Models via Consistency Alignment](llm-and-security/llm-defense/improving-the-robustness-of-large-language-models-via-consistency-alignment.md)
    * [SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding](llm-and-security/llm-defense/safedecoding-defending-against-jailbreak-attacks-via-safety-aware-decoding.md)
    * [Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization](llm-and-security/llm-defense/defending-large-language-models-against-jailbreaking-attacks-through-goal-prioritization.md)
    * [Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks](llm-and-security/llm-defense/defending-pre-trained-language-models-as-few-shot-learners-against-backdoor-attacks.md)
    * [LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors](llm-and-security/llm-defense/lmsanitator-defending-prompt-tuning-against-task-agnostic-backdoors.md)
    * [Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language](llm-and-security/llm-defense/diffusion-theory-as-a-scalpel-detecting-and-purifying-poisonous-dimensions-in-pre-trained-language.md)
    * [Analyzing And Editing Inner Mechanisms of Backdoored Language Models](llm-and-security/llm-defense/analyzing-and-editing-inner-mechanisms-of-backdoored-language-models.md)
    * [Setting the Trap: Capturing and Defeating Backdoors in Pretrained Language Models through Honeypots](llm-and-security/llm-defense/setting-the-trap-capturing-and-defeating-backdoors-in-pretrained-language-models-through-honeypots.md)
    * [ROBUSTIFYING LANGUAGE MODELS WITH TESTTIME ADAPTATION](llm-and-security/llm-defense/robustifying-language-models-with-testtime-adaptation.md)
    * [Jailbreaker in Jail: Moving Target Defense for Large Language](llm-and-security/llm-defense/jailbreaker-in-jail-moving-target-defense-for-large-language.md)
    * [DETECTING LANGUAGE MODEL ATTACKS WITH PERPLEXITY](llm-and-security/llm-defense/detecting-language-model-attacks-with-perplexity.md)
    * [Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation an](llm-and-security/llm-defense/adversarial-fine-tuning-of-language-models-an-iterative-optimisation-approach-for-the-generation-an.md)
    * [From Adversarial Arms Race to Model-centric Evaluation Motivating a Unified Automatic Robustness Eva](llm-and-security/llm-defense/from-adversarial-arms-race-to-model-centric-evaluation-motivating-a-unified-automatic-robustness-eva.md)
    * [LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked](llm-and-security/llm-defense/llm-self-defense-by-self-examination-llms-know-they-are-being-tricked.md)
    * [Intention Analysis Makes LLMs A Good Jailbreak Defender](llm-and-security/llm-defense/intention-analysis-makes-llms-a-good-jailbreak-defender.md)
    * [Defending Against Disinformation Attacks in Open-Domain Question Answering](llm-and-security/llm-defense/defending-against-disinformation-attacks-in-open-domain-question-answering.md)
    * [Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning](llm-and-security/llm-defense/pruning-for-protection-increasing-jailbreak-resistance-in-aligned-llms-without-fine-tuning-1.md)
    * [Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landsc](llm-and-security/llm-defense/gradient-cuff-detecting-jailbreak-attacks-on-large-language-models-by-exploring-refusal-loss-landsc.md)
    * [Round Trip Translation Defence against Large Language Model Jailbreaking Attacks](llm-and-security/llm-defense/round-trip-translation-defence-against-large-language-model-jailbreaking-attacks.md)
    * [How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?](llm-and-security/llm-defense/how-should-pre-trained-language-models-be-fine-tuned-towards-adversarial-robustness.md)
    * [SELF-GUARD: Empower the LLM to Safeguard Itself](llm-and-security/llm-defense/self-guard-empower-the-llm-to-safeguard-itself.md)
    * [Intention Analysis Makes LLMs A Good Jailbreak Defender](llm-and-security/llm-defense/intention-analysis-makes-llms-a-good-jailbreak-defender-1.md)
    * [Jatmo: Prompt Injection Defense by Task-Specific Finetuning](llm-and-security/llm-defense/jatmo-prompt-injection-defense-by-task-specific-finetuning.md)
    * [Precisely the Point: Adversarial Augmentations for Faithful and Informative Text Generation](llm-and-security/llm-defense/precisely-the-point-adversarial-augmentations-for-faithful-and-informative-text-generation.md)
    * [Adversarial Text Purification: A Large Language Model Approach for Defense](llm-and-security/llm-defense/adversarial-text-purification-a-large-language-model-approach-for-defense.md)
    * [Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning](llm-and-security/llm-defense/studious-bob-fight-back-against-jailbreaking-via-prompt-adversarial-tuning-1.md)
    * [Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks](llm-and-security/llm-defense/robust-prompt-optimization-for-defending-language-models-against-jailbreaking-attacks-1.md)
    * [Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning](llm-and-security/llm-defense/defending-against-weight-poisoning-backdoor-attacks-for-parameter-efficient-fine-tuning-1.md)
    * [Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Application](llm-and-security/llm-defense/signed-prompt-a-new-approach-to-prevent-prompt-injection-attacks-against-llm-integrated-application.md)
    * [Is the System Message Really Important to Jailbreaks in Large Language Models?](llm-and-security/llm-defense/is-the-system-message-really-important-to-jailbreaks-in-large-language-models.md)
    * [AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts](llm-and-security/llm-defense/aegis-online-adaptive-ai-content-safety-moderation-with-ensemble-of-llm-experts.md)
    * [Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge](llm-and-security/llm-defense/eraser-jailbreaking-defense-in-large-language-models-via-unlearning-harmful-knowledge.md)
* [GNN](gnn/README.md)
  * [GNN Basic](gnn/gnn-basic.md)
* [AI图像生成与攻防技术学习笔记](ai-tu-xiang-sheng-cheng-yu-gong-fang-ji-shu-xue-xi-bi-ji.md)
